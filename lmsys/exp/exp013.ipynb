{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目的\n",
    "gemmaを使う、元のNotebookにtokenizer以外の設定を合わせる\n",
    "\n",
    "https://www.kaggle.com/code/emiz6413/training-gemma-2-9b-4-bit-qlora-fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path setting\n",
    "EXP_NAME = \"e013-use-gemma-use-org-cfg\"\n",
    "MODEL_NAME = \"unsloth/gemma-2-9b-it-bnb-4bit\"\n",
    "COMPETITION_NAME = \"lmsys\"\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "DATASET_NAME = f\"{EXP_NAME}-{MODEL_NAME.split('/')[-1]}\"\n",
    "MODEL_OUTPUT_PATH = f\"trained_models/{EXP_NAME}\"\n",
    "\n",
    "# experiment parameter\n",
    "DEBUG = False\n",
    "TRAINING = True\n",
    "UPLOAD_DATA_TO_S3 = True\n",
    "UPLOAD_DATA_TO_KAGGLE = True\n",
    "REMOVE_LOCAL_FILE = False\n",
    "WANDB = True\n",
    "USE_FOLD = 2\n",
    "USE_DATA_RATE = 0.1\n",
    "\n",
    "# model parameter\n",
    "TRAINING_MAX_LENGTH = 1024 # 512\n",
    "INFERENCE_MAX_LENGTH = 1024  # 1536\n",
    "SEED = 42\n",
    "VALID_DATA_SIZE = 0.3\n",
    "EPOCH = 1\n",
    "LR = 2e-04\n",
    "TRAIN_BS = 32\n",
    "GRAD_ACC_STEP= 128 // TRAIN_BS # 仮想的なバッチサイズはTRAIN_BS * GRAD_ACC_STEPとなる\n",
    "EVAL_BS = 32\n",
    "NUM_LABELS = 3\n",
    "\n",
    "FREEZE_LAYERS = (\n",
    "    16  # there're 42 layers in total, we don't add adapters to the first 16 layers\n",
    ")\n",
    "\n",
    "# rola parameter\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = LORA_R * 2\n",
    "LORA_DROPOUT = 0.05\n",
    "LORA_BIAS = \"none\"\n",
    "\n",
    "RESUME_FROM_CHECKPOINT= True # 途中から再開する場合はTrueにする\n",
    "# TRAINED_MODEL_PATH = \"lmsys/trained_models/e006-use-concat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 15 12:43:49 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   39C    P8    22W / 300W |      1MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.7\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/lmsys/lmsys/exp\n",
      "Jupyter Lab!\n",
      "../../data\n",
      "/notebooks/lmsys/lmsys/exp\n",
      "Jupyter Lab!\n",
      "../../trained_models/e013-use-gemma-use-org-cfg\n"
     ]
    }
   ],
   "source": [
    "def resolve_path(base_path: str) -> str:\n",
    "    import os\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    print(cwd)\n",
    "    if cwd == f\"/notebooks\":\n",
    "        print(\"Jupyter Kernel By VSCode!\")\n",
    "        return f\"/notebooks/{COMPETITION_NAME}/{base_path}\"\n",
    "    elif cwd == f\"/notebooks/{COMPETITION_NAME}\":\n",
    "        print(\"nohup!\")\n",
    "        return base_path\n",
    "    elif cwd == f\"/notebooks/{COMPETITION_NAME}/{COMPETITION_NAME}/exp\":\n",
    "        print(\"Jupyter Lab!\")\n",
    "        return f\"../../{base_path}\"\n",
    "    elif cwd == f\"/content\":\n",
    "        print(\"Google Colab!\")\n",
    "        return f\"/content/drive/MyDrive/Kaggle/{COMPETITION_NAME}/{base_path}\"\n",
    "    else:\n",
    "        raise Exception(\"Unknown environment\")\n",
    "\n",
    "\n",
    "DATA_PATH = resolve_path(DATA_PATH)\n",
    "print(DATA_PATH)\n",
    "MODEL_OUTPUT_PATH = resolve_path(MODEL_OUTPUT_PATH)\n",
    "print(MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate_dataset_name(dataset_name: str) -> None:\n",
    "    if len(dataset_name) < 6 or len(dataset_name) > 50:\n",
    "        raise Exception(\n",
    "            f\"データセットの文字列は6~50文字にしてください。現在{len(DATASET_NAME)}文字\"\n",
    "        )\n",
    "    if \"_\" in dataset_name:\n",
    "        raise Exception(\"datasetの名称に_の使用は禁止です\")\n",
    "\n",
    "\n",
    "validate_dataset_name(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient 2.0.6 requires attrs<=19, but you have attrs 23.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient 2.0.6 requires attrs<=19, but you have attrs 23.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qq polars==1.0.0\n",
    "%pip install -qq transformers==4.42.3\n",
    "%pip install -qq datasets==2.20.0\n",
    "%pip install -qq evaluate==0.4.2\n",
    "%pip install -qq seqeval==1.2.2\n",
    "%pip install -qq accelerate==0.32.0\n",
    "%pip install -qq python-dotenv==1.0.1\n",
    "%pip install -qq wandb==0.17.4\n",
    "%pip install -qq bitsandbytes==0.43.1\n",
    "%pip install -qq accelerate==0.32.0\n",
    "%pip install -qq peft==0.11.1\n",
    "\n",
    "# formatter\n",
    "%pip install -qq black isort\n",
    "\n",
    "%pip install -qq kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-15 12:44:48.349388: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-15 12:44:48.349447: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-15 12:44:48.350607: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-15 12:44:48.357503: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-15 12:44:49.412768: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import ast\n",
    "import json\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, LayerNorm, MSELoss\n",
    "import wandb\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    DatasetDict,\n",
    "    Value,\n",
    "    concatenate_datasets,\n",
    "    load_dataset,\n",
    "    ClassLabel,\n",
    ")\n",
    "from tokenizers import AddedToken\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import log_loss\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    Gemma2ForSequenceClassification,\n",
    "    GemmaTokenizerFast,\n",
    "    Gemma2Config,\n",
    "    PreTrainedTokenizerBase,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "NUM_PROC = os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import evaluate\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "import peft\n",
    "\n",
    "assert transformers.__version__ == \"4.42.3\"\n",
    "assert datasets.__version__ == \"2.20.0\"\n",
    "assert evaluate.__version__ == \"0.4.2\"\n",
    "assert bitsandbytes.__version__ == \"0.43.1\"\n",
    "assert accelerate.__version__ == \"0.32.0\"\n",
    "assert peft.__version__ == \"0.11.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seed the same seed to all\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(f\"{DATA_PATH}/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-15T09:34:04.684580Z",
     "iopub.status.busy": "2024-07-15T09:34:04.684389Z",
     "iopub.status.idle": "2024-07-15T09:34:06.098878Z",
     "shell.execute_reply": "2024-07-15T09:34:06.098363Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wandb'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if WANDB:\n",
    "    wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n",
    "    wandb.init(project=COMPETITION_NAME, name=EXP_NAME)\n",
    "    REPORT_TO = \"wandb\"\n",
    "else:\n",
    "    REPORT_TO = \"none\"\n",
    "\n",
    "REPORT_TO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import & Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T09:34:06.100622Z",
     "iopub.status.busy": "2024-07-15T09:34:06.100497Z",
     "iopub.status.idle": "2024-07-15T09:34:06.129355Z",
     "shell.execute_reply": "2024-07-15T09:34:06.128727Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(f\"{DATA_PATH}/label_stratified_fold.json\") as f:\n",
    "    label_stratified_fold = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T09:34:06.131683Z",
     "iopub.status.busy": "2024-07-15T09:34:06.131509Z",
     "iopub.status.idle": "2024-07-15T09:34:07.755184Z",
     "shell.execute_reply": "2024-07-15T09:34:07.754557Z"
    }
   },
   "outputs": [],
   "source": [
    "train = (\n",
    "    pl.read_csv(f\"{DATA_PATH}/train.csv\")\n",
    "    .with_columns(\n",
    "        pl.col(\"prompt\").str.json_decode(),\n",
    "        pl.col(\"response_a\").str.json_decode(),\n",
    "        pl.col(\"response_b\").str.json_decode(),\n",
    "    )\n",
    "    .with_columns(  # 長さの情報を追加する\n",
    "        pl.col(\"prompt\")\n",
    "        .map_elements(lambda x: len(x), return_dtype=pl.Int64)\n",
    "        .alias(\"len_prompt\"),\n",
    "        pl.col(\"response_a\")\n",
    "        .map_elements(lambda x: len(x), return_dtype=pl.Int64)\n",
    "        .alias(\"len_response_a\"),\n",
    "        pl.col(\"response_b\")\n",
    "        .map_elements(lambda x: len(x), return_dtype=pl.Int64)\n",
    "        .alias(\"len_response_b\"),\n",
    "    )\n",
    "    .with_columns(  # 最後のレスポンスのみを取得する\n",
    "        pl.col(\"prompt\")\n",
    "        .map_elements(lambda x: x[-1], return_dtype=pl.String)\n",
    "        .alias(\"last_prompt\"),\n",
    "        pl.col(\"response_a\")\n",
    "        .map_elements(lambda x: x[-1], return_dtype=pl.String)\n",
    "        .alias(\"last_response_a\"),\n",
    "        pl.col(\"response_b\")\n",
    "        .map_elements(lambda x: x[-1], return_dtype=pl.String)\n",
    "        .alias(\"last_response_b\"),\n",
    "    )\n",
    "    .with_columns(  # 最後のレスポンスがNoneの場合を空文字にする、約60件程度\n",
    "        pl.col(\"last_response_a\").fill_null(\"\"),\n",
    "        pl.col(\"last_response_b\").fill_null(\"\"),\n",
    "    )\n",
    "    .with_columns(  # labelを付与する\n",
    "        pl.when(pl.col(\"winner_model_a\") == 1)\n",
    "        .then(0)\n",
    "        .when(pl.col(\"winner_model_b\") == 1)\n",
    "        .then(1)\n",
    "        .when(pl.col(\"winner_tie\") == 1)\n",
    "        .then(2)\n",
    "        .alias(\"label\"),\n",
    "    )\n",
    "    .select(  # 元のprompt, responseを削除する\n",
    "        pl.exclude([\"prompt\", \"response_a\", \"response_b\"])\n",
    "    )\n",
    "    .with_columns(  # foldを追加する\n",
    "        pl.col(\"id\").replace(label_stratified_fold).alias(\"fold\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T09:34:07.757552Z",
     "iopub.status.busy": "2024-07-15T09:34:07.757346Z",
     "iopub.status.idle": "2024-07-15T09:34:07.759902Z",
     "shell.execute_reply": "2024-07-15T09:34:07.759393Z"
    }
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    train = train.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T09:34:07.761865Z",
     "iopub.status.busy": "2024-07-15T09:34:07.761714Z",
     "iopub.status.idle": "2024-07-15T09:34:08.293646Z",
     "shell.execute_reply": "2024-07-15T09:34:08.293036Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_polars(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-15T09:34:08.296105Z",
     "iopub.status.busy": "2024-07-15T09:34:08.295910Z",
     "iopub.status.idle": "2024-07-15T09:34:08.299320Z",
     "shell.execute_reply": "2024-07-15T09:34:08.298841Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie', 'len_prompt', 'len_response_a', 'len_response_b', 'last_prompt', 'last_response_a', 'last_response_b', 'label', 'fold'],\n",
       "    num_rows: 57477\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T09:34:08.301015Z",
     "iopub.status.busy": "2024-07-15T09:34:08.300861Z",
     "iopub.status.idle": "2024-07-15T09:34:08.317808Z",
     "shell.execute_reply": "2024-07-15T09:34:08.317147Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 計算を早くするために、データを減らす\n",
    "if not DEBUG:\n",
    "    train_dataset = train_dataset.shuffle().select(\n",
    "        range(\n",
    "            int(len(train_dataset) * USE_DATA_RATE)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T09:34:08.319721Z",
     "iopub.status.busy": "2024-07-15T09:34:08.319552Z",
     "iopub.status.idle": "2024-07-15T09:34:08.322422Z",
     "shell.execute_reply": "2024-07-15T09:34:08.321943Z"
    }
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA, # low-rankマトリクスのスケーリングファクター\n",
    "    # only target self-attention\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    layers_to_transform=[i for i in range(42) if i >= FREEZE_LAYERS],\n",
    "    lora_dropout=LORA_DROPOUT, # LoRAレイヤーのドロップアウト確率\n",
    "    bias=LORA_BIAS,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-15T09:34:08.323965Z",
     "iopub.status.busy": "2024-07-15T09:34:08.323814Z",
     "iopub.status.idle": "2024-07-15T09:34:09.469240Z",
     "shell.execute_reply": "2024-07-15T09:34:09.468418Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36cc7d1a6a949fe925b4c655dab8bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85bf42ea76aa420fafcb853ad0a19de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ffe0b82704d4f58ab87afdad321c01a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095fda6e23214c9b90f124653bd5a083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer = GemmaTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.add_eos_token = True  # We'll add <eos> at the end\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"[SEP]\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-15T09:34:09.471115Z",
     "iopub.status.busy": "2024-07-15T09:34:09.470950Z",
     "iopub.status.idle": "2024-07-15T09:39:16.044605Z",
     "shell.execute_reply": "2024-07-15T09:39:16.044066Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5dc9077d1ce45799f40c941b9350af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab0646c147e44a2af73a61b23cb88b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=16)\n",
    "print(model)\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T09:39:16.046885Z",
     "iopub.status.busy": "2024-07-15T09:39:16.046431Z",
     "iopub.status.idle": "2024-07-15T09:39:16.049601Z",
     "shell.execute_reply": "2024-07-15T09:39:16.049011Z"
    }
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-15T09:39:16.051484Z",
     "iopub.status.busy": "2024-07-15T09:39:16.051328Z",
     "iopub.status.idle": "2024-07-15T09:39:34.930885Z",
     "shell.execute_reply": "2024-07-15T09:39:34.930037Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d4761368984caea83199a29dc1c170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/5747 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(examples, max_token_length: int):\n",
    "    separator = \" [SEP] \"\n",
    "\n",
    "    joined_text = (\n",
    "        examples[\"last_prompt\"]\n",
    "        + separator\n",
    "        + examples[\"last_response_a\"]\n",
    "        + separator\n",
    "        + examples[\"last_response_b\"]\n",
    "    )\n",
    "\n",
    "    return tokenizer(\n",
    "        joined_text,\n",
    "        max_length=max_token_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize,\n",
    "    batched=False,\n",
    "    fn_kwargs={\"max_token_length\": TRAINING_MAX_LENGTH},\n",
    "    num_proc=NUM_PROC,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T09:39:34.934030Z",
     "iopub.status.busy": "2024-07-15T09:39:34.933839Z",
     "iopub.status.idle": "2024-07-15T09:39:34.938214Z",
     "shell.execute_reply": "2024-07-15T09:39:34.937821Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-15T09:39:34.940453Z",
     "iopub.status.busy": "2024-07-15T09:39:34.939916Z",
     "iopub.status.idle": "2024-07-15T09:39:34.943270Z",
     "shell.execute_reply": "2024-07-15T09:39:34.942935Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie', 'len_prompt', 'len_response_a', 'len_response_b', 'last_prompt', 'last_response_a', 'last_response_b', 'label', 'fold', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 5747\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-15T09:39:34.945575Z",
     "iopub.status.busy": "2024-07-15T09:39:34.944624Z",
     "iopub.status.idle": "2024-07-15T09:39:39.573835Z",
     "shell.execute_reply": "2024-07-15T09:39:39.572952Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4079714d59c44f393699cc6310aceca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=8):   0%|          | 0/5747 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9740a0b15d74e288f5151de25f2c3ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=8):   0%|          | 0/5747 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def to_train_valid(ds):\n",
    "#     return DatasetDict({\"train\": ds[\"train\"], \"valid\": ds[\"test\"]})\n",
    "\n",
    "\n",
    "# train_dataset = train_dataset.cast_column(\"label\", ClassLabel(num_classes=NUM_LABELS))\n",
    "\n",
    "# train_valid_dataset = to_train_valid(\n",
    "#     (\n",
    "#         train_dataset.train_test_split(\n",
    "#             test_size=VALID_DATA_SIZE, seed=SEED, stratify_by_column=\"label\"\n",
    "#         )\n",
    "#     )\n",
    "# )\n",
    "\n",
    "train_valid_dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": train_dataset.filter(\n",
    "            lambda x: x[\"fold\"] != USE_FOLD, num_proc=NUM_PROC\n",
    "        ),\n",
    "        \"valid\": train_dataset.filter(\n",
    "            lambda x: x[\"fold\"] == USE_FOLD, num_proc=NUM_PROC\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T09:39:39.577319Z",
     "iopub.status.busy": "2024-07-15T09:39:39.576587Z",
     "iopub.status.idle": "2024-07-15T09:39:39.582053Z",
     "shell.execute_reply": "2024-07-15T09:39:39.581411Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T09:39:39.584345Z",
     "iopub.status.busy": "2024-07-15T09:39:39.584043Z",
     "iopub.status.idle": "2024-07-15T09:39:39.588459Z",
     "shell.execute_reply": "2024-07-15T09:39:39.587885Z"
    }
   },
   "outputs": [],
   "source": [
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     preds_prob = softmax(predictions, axis=-1)\n",
    "#     return {\"log_loss\": log_loss(labels, preds_prob)}\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred) -> dict:\n",
    "    preds, labels = eval_pred\n",
    "    preds_prob = softmax(preds, axis=-1)\n",
    "    return {\n",
    "        \"log_loss\": log_loss(y_true=labels, y_pred=preds_prob),\n",
    "        \"acc\": accuracy_score(y_true=labels, y_pred=preds.argmax(-1)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T09:39:39.590350Z",
     "iopub.status.busy": "2024-07-15T09:39:39.590169Z",
     "iopub.status.idle": "2024-07-15T09:39:39.617863Z",
     "shell.execute_reply": "2024-07-15T09:39:39.617295Z"
    }
   },
   "outputs": [],
   "source": [
    "# スケジューラの設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_PATH,\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=TRAIN_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEP,\n",
    "    eval_accumulation_steps=GRAD_ACC_STEP,\n",
    "    per_device_eval_batch_size=EVAL_BS,\n",
    "    num_train_epochs=EPOCH,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1/3,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1/3,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=2,\n",
    "    seed=SEED,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\", # \"linear\", # \"constant_with_warmup\",\n",
    "    report_to=REPORT_TO,\n",
    "    run_name=EXP_NAME,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_8bit\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_valid_dataset[\"train\"],\n",
    "    # train_dataset=ConcatDataset(train_valid_dataset[\"train\"]),\n",
    "    eval_dataset=train_valid_dataset[\"valid\"],\n",
    "    # eval_dataset=ConcatDataset(train_valid_dataset[\"valid\"]),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-15T09:39:39.620123Z",
     "iopub.status.busy": "2024-07-15T09:39:39.619932Z",
     "iopub.status.idle": "2024-07-15T09:40:04.045859Z",
     "shell.execute_reply": "2024-07-15T09:40:04.045058Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/30 : < :, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if TRAINING:\n",
    "    # モデルの学習\n",
    "    trainer.train(\n",
    "        resume_from_checkpoint = RESUME_FROM_CHECKPOINT if RESUME_FROM_CHECKPOINT else None\n",
    "    )\n",
    "    # ログの保存に利用したストレージを削除\n",
    "    os.system(f\"rm -rf {MODEL_OUTPUT_PATH}/checkpoint-*\")\n",
    "    # モデルの保存\n",
    "    trainer.save_model(MODEL_OUTPUT_PATH)\n",
    "else:\n",
    "    # TRAINED_MODEL_PATHを用いて、学習済のモデルを読み込む\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        TRAINED_MODEL_PATH,\n",
    "        num_labels=NUM_LABELS,\n",
    "    )\n",
    "    # model = CustomDebertaSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        \".\",\n",
    "        per_device_eval_batch_size=4,\n",
    "        report_to=\"none\",\n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
    "TODO: この　Warningが問題ないのかを調べる\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# valid_datasetの作成・保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-15T09:40:04.049393Z",
     "iopub.status.busy": "2024-07-15T09:40:04.048765Z",
     "iopub.status.idle": "2024-07-15T09:48:14.614052Z",
     "shell.execute_reply": "2024-07-15T09:48:14.613406Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ea9cd5d721474f88201caa6b1a5374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=8):   0%|          | 0/5747 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a196313e4543a9b65505fb32853e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/1867 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6aeaccb2234ae69d8549e23d76d60f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1867 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ee0f477b6a45c397e83dfdfd201279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1867 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TRAININGをINFERRENCEでMAX_TOKENを変えるために、validを作り直す\n",
    "valid_dataset = train_dataset.filter(\n",
    "    lambda example: example[\"id\"] in train_valid_dataset[\"valid\"][\"id\"],\n",
    "    num_proc=NUM_PROC,\n",
    ")\n",
    "\n",
    "valid_dataset = valid_dataset.map(\n",
    "    tokenize,\n",
    "    batched=False,\n",
    "    fn_kwargs={\"max_token_length\": INFERENCE_MAX_LENGTH},\n",
    "    num_proc=NUM_PROC,\n",
    ")\n",
    "\n",
    "# valid_dataset = valid_dataset.map(\n",
    "#     tokenize,\n",
    "#     batched=False,\n",
    "#     fn_kwargs={\n",
    "#         \"suffix\": \"a\",\n",
    "#         \"max_token_length\": INFERENCE_MAX_LENGTH\n",
    "#     },\n",
    "#     num_proc=NUM_PROC,\n",
    "# ).map(\n",
    "#     tokenize,\n",
    "#     batched=False,\n",
    "#     fn_kwargs={\n",
    "#         \"suffix\": \"b\",\n",
    "#         \"max_token_length\": INFERENCE_MAX_LENGTH\n",
    "#     },\n",
    "#     num_proc=NUM_PROC,\n",
    "# )\n",
    "\n",
    "\n",
    "def add_valid_pred(example, idx, valid_pred):\n",
    "    example[\"valid_pred\"] = valid_pred[idx]\n",
    "    return example\n",
    "\n",
    "\n",
    "valid_dataset = train_valid_dataset[\"valid\"]\n",
    "\n",
    "valid_pred = softmax(trainer.predict(valid_dataset).predictions, axis=-1)\n",
    "# valid_pred = softmax(trainer.predict(ConcatDataset(valid_dataset)).predictions, axis=-1)\n",
    "\n",
    "np.save(f\"{MODEL_OUTPUT_PATH}/valid_prediction.npy\", valid_pred)\n",
    "\n",
    "valid_dataset = valid_dataset.map(\n",
    "    add_valid_pred, with_indices=True, fn_kwargs={\"valid_pred\": valid_pred}\n",
    ")\n",
    "\n",
    "valid_dataset.save_to_disk(f\"{MODEL_OUTPUT_PATH}/valid_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T09:48:14.616147Z",
     "iopub.status.busy": "2024-07-15T09:48:14.615964Z",
     "iopub.status.idle": "2024-07-15T09:48:14.622790Z",
     "shell.execute_reply": "2024-07-15T09:48:14.622255Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_score = log_loss(valid_dataset[\"label\"], valid_pred)\n",
    "print(f\"CV Score: {cv_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T09:48:14.624499Z",
     "iopub.status.busy": "2024-07-15T09:48:14.624333Z",
     "iopub.status.idle": "2024-07-15T09:48:14.630436Z",
     "shell.execute_reply": "2024-07-15T09:48:14.629981Z"
    }
   },
   "outputs": [],
   "source": [
    "# output_textを保存\n",
    "with open(f\"{MODEL_OUTPUT_PATH}/cv_score.txt\", \"w\") as f:\n",
    "    f.write(str(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWSへのアップロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T09:48:14.632015Z",
     "iopub.status.busy": "2024-07-15T09:48:14.631868Z",
     "iopub.status.idle": "2024-07-15T09:49:53.942497Z",
     "shell.execute_reply": "2024-07-15T09:49:53.941734Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# S3へのアップロード\n",
    "# TODO: colabでは動かないため直す\n",
    "if not DEBUG and UPLOAD_DATA_TO_S3:\n",
    "    # uninstall\n",
    "    !sudo rm /usr/bin/aws\n",
    "    !sudo rm /usr/bin/aws_completer\n",
    "    !sudo rm -rf /usr/local/aws-cli\n",
    "\n",
    "    # install\n",
    "    !curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
    "    !unzip -o -qq awscliv2.zip\n",
    "    !sudo ./aws/install --update\n",
    "\n",
    "    # upload\n",
    "    output_name = MODEL_OUTPUT_PATH.split(\"/\")[-1]\n",
    "    os.system(\n",
    "        f\"aws s3 cp --recursive {MODEL_OUTPUT_PATH} s3://{COMPETITION_NAME}/trained_model/{output_name}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T09:49:53.944575Z",
     "iopub.status.busy": "2024-07-15T09:49:53.944384Z",
     "iopub.status.idle": "2024-07-15T09:49:53.947522Z",
     "shell.execute_reply": "2024-07-15T09:49:53.947052Z"
    }
   },
   "outputs": [],
   "source": [
    "# ダウンロード（参考）\n",
    "# !sudo rm /usr/bin/aws\n",
    "# !sudo rm /usr/bin/aws_completer\n",
    "# !sudo rm -rf /usr/local/aws-cli\n",
    "\n",
    "# !curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
    "# !unzip -o -qq awscliv2.zip\n",
    "# !sudo ./aws/install --update\n",
    "\n",
    "# !aws s3 cp --recursive s3://automated-essay-scoring/trained_model/e005-regression /notebooks/automated_essay_scoring/trained_models/e005-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Datasetへのupload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.system(\"mkdir -p ~/.kaggle/\")\n",
    "os.system(f\"cp {DATA_PATH}/kaggle.json ~/.kaggle/\")\n",
    "os.system(\"chmod 600 ~/.kaggle/kaggle.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Dataset name:e013-use-gemma-use-org-cfg-gemma-2-9b-it-bnb-4bit, output_dir:../../trained_models/e013-use-gemma-use-org-cfg\n",
      "Starting upload for file cv_score.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17.0/17.0 [00:00<00:00, 57.2B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: cv_score.txt (17B)\n",
      "Starting upload for file tokenizer.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.04M/4.04M [00:00<00:00, 10.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: tokenizer.model (4MB)\n",
      "Starting upload for file README.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.98k/4.98k [00:00<00:00, 18.2kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: README.md (5KB)\n",
      "Starting upload for file training_args.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.12k/5.12k [00:00<00:00, 18.7kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: training_args.bin (5KB)\n",
      "Starting upload for file tokenizer.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16.7M/16.7M [00:00<00:00, 33.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: tokenizer.json (17MB)\n",
      "Starting upload for file valid_prediction.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22.0k/22.0k [00:00<00:00, 82.2kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: valid_prediction.npy (22KB)\n",
      "Starting upload for file valid_dataset.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13.6M/13.6M [00:00<00:00, 30.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: valid_dataset.tar (14MB)\n",
      "Starting upload for file adapter_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 907/907 [00:00<00:00, 3.34kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: adapter_config.json (907B)\n",
      "Starting upload for file added_tokens.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22.0/22.0 [00:00<00:00, 77.4B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: added_tokens.json (22B)\n",
      "Starting upload for file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39.8k/39.8k [00:00<00:00, 150kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: tokenizer_config.json (40KB)\n",
      "Starting upload for file special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 731/731 [00:00<00:00, 2.67kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: special_tokens_map.json (731B)\n",
      "Starting upload for file adapter_model.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3.45G/3.45G [00:38<00:00, 95.2MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: adapter_model.safetensors (3GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not DEBUG and UPLOAD_DATA_TO_KAGGLE:\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "    def dataset_create_new(dataset_name: str, upload_dir: str):\n",
    "        # if \"_\" in dataset_name:\n",
    "        #     raise ValueError(\"datasetの名称に_の使用は禁止です\")\n",
    "        dataset_metadata = {}\n",
    "        dataset_metadata[\"id\"] = f\"sinchir0/{dataset_name}\"\n",
    "        dataset_metadata[\"licenses\"] = [{\"name\": \"CC0-1.0\"}]\n",
    "        dataset_metadata[\"title\"] = dataset_name\n",
    "        with open(os.path.join(upload_dir, \"dataset-metadata.json\"), \"w\") as f:\n",
    "            json.dump(dataset_metadata, f, indent=4)\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode=\"tar\")\n",
    "\n",
    "    print(f\"Create Dataset name:{DATASET_NAME}, output_dir:{MODEL_OUTPUT_PATH}\")\n",
    "    dataset_create_new(dataset_name=DATASET_NAME, upload_dir=MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ローカルからのデータの削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not DEBUG and REMOVE_LOCAL_FILE:\n",
    "    # ローカルからは削除\n",
    "    os.system(f\"rm -rf {MODEL_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if WANDB:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish Notebook!\n"
     ]
    }
   ],
   "source": [
    "print(\"finish Notebook!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
