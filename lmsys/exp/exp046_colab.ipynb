{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sinchir0/lmsys/blob/main/lmsys/exp/exp046_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzT66iezisGu"
      },
      "source": [
        "# 目的\n",
        "gemmaを使う、full-fine-tune、1536\n",
        "\n",
        "https://www.kaggle.com/code/emiz6413/training-gemma-2-9b-4-bit-qlora-fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iZIuNyc2v2l",
        "outputId": "e2cc92ba-7643-422b-971e-d5dd82c4ede6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7YkHHYsisGw"
      },
      "outputs": [],
      "source": [
        "# path setting\n",
        "EXP_NAME = \"e046-full-fine-tune-1536\"\n",
        "MODEL_NAME = \"unsloth/gemma-2-9b-it-bnb-4bit\"\n",
        "COMPETITION_NAME = \"lmsys\"\n",
        "\n",
        "DATA_PATH = \"data\"\n",
        "DATASET_NAME = f\"{EXP_NAME}-{MODEL_NAME.split('/')[-1]}\"\n",
        "MODEL_OUTPUT_PATH = f\"trained_models/{EXP_NAME}\"\n",
        "\n",
        "# experiment parameter\n",
        "DEBUG = False\n",
        "TRAINING = True\n",
        "UPLOAD_DATA_TO_S3 = True\n",
        "UPLOAD_DATA_TO_KAGGLE = True\n",
        "REMOVE_LOCAL_FILE = False\n",
        "WANDB = True\n",
        "USE_FOLD = 2\n",
        "USE_DATA_RATE = 1.0\n",
        "VALID_DATA_SIZE = 3000\n",
        "\n",
        "# model parameter\n",
        "TRAINING_MAX_LENGTH = 1536 # 512\n",
        "INFERENCE_MAX_LENGTH = 1536\n",
        "SEED = 42\n",
        "EPOCH = 1\n",
        "LR = 2e-04\n",
        "TRAIN_BS = 4 # 16\n",
        "GRAD_ACC_STEP= 128 // TRAIN_BS # 仮想的なバッチサイズはTRAIN_BS * GRAD_ACC_STEPとなる\n",
        "EVAL_BS = 4 # 16\n",
        "NUM_LABELS = 3\n",
        "\n",
        "FREEZE_LAYERS = (\n",
        "    0  # there're 42 layers in total, we don't add adapters to the first 16 layers\n",
        ")\n",
        "\n",
        "# rola parameter\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = LORA_R * 2\n",
        "LORA_DROPOUT = 0.05\n",
        "LORA_BIAS = \"none\"\n",
        "\n",
        "RESUME_FROM_CHECKPOINT= True # 途中から再開する場合はTrueにする\n",
        "# TRAINED_MODEL_PATH = \"lmsys/trained_models/e006-use-concat\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-1rTdTmisGy",
        "outputId": "ce096b0c-8eeb-40d9-9fad-6632ad3b493c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Jul 22 09:56:34 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0              47W / 400W |  22271MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdIcXQzqisGz",
        "outputId": "34af4f15-61a3-4c2b-bc13-d5396118f4e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeHMykyGisG0",
        "outputId": "dfc777ab-0471-4239-d854-36f401359104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Google Colab!\n",
            "/content/drive/MyDrive/Kaggle/lmsys/data\n",
            "/content\n",
            "Google Colab!\n",
            "/content/drive/MyDrive/Kaggle/lmsys/trained_models/e046-full-fine-tune-1536\n"
          ]
        }
      ],
      "source": [
        "def resolve_path(base_path: str) -> str:\n",
        "    import os\n",
        "\n",
        "    cwd = os.getcwd()\n",
        "    print(cwd)\n",
        "    if cwd == f\"/notebooks\":\n",
        "        print(\"Jupyter Kernel By VSCode!\")\n",
        "        return \"kernel\", f\"/notebooks/{COMPETITION_NAME}/{base_path}\"\n",
        "    elif cwd == f\"/notebooks/{COMPETITION_NAME}\":\n",
        "        print(\"nohup!\")\n",
        "        return base_path\n",
        "    elif cwd == f\"/notebooks/{COMPETITION_NAME}/{COMPETITION_NAME}/exp\":\n",
        "        print(\"Jupyter Lab!\")\n",
        "        return \"nohup\", f\"../../{base_path}\"\n",
        "    elif cwd == f\"/content\":\n",
        "        print(\"Google Colab!\")\n",
        "        return \"colab\", f\"/content/drive/MyDrive/Kaggle/{COMPETITION_NAME}/{base_path}\"\n",
        "    elif cwd.startswith(\"/home/shinichiro.saito\"):\n",
        "        print(\"GCP!\")\n",
        "        return \"GCP\", f\"/home/shinichiro.saito/{COMPETITION_NAME}/{base_path}\"\n",
        "    else:\n",
        "        raise Exception(\"Unknown environment\")\n",
        "\n",
        "\n",
        "ENV_NAME, DATA_PATH = resolve_path(DATA_PATH)\n",
        "print(DATA_PATH)\n",
        "_, MODEL_OUTPUT_PATH = resolve_path(MODEL_OUTPUT_PATH)\n",
        "print(MODEL_OUTPUT_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E78wS3ypisG0"
      },
      "outputs": [],
      "source": [
        "def validate_dataset_name(dataset_name: str) -> None:\n",
        "    if len(dataset_name) < 6 or len(dataset_name) > 50:\n",
        "        raise Exception(\n",
        "            f\"データセットの文字列は6~50文字にしてください。現在{len(DATASET_NAME)}文字\"\n",
        "        )\n",
        "    if \"_\" in dataset_name:\n",
        "        raise Exception(\"datasetの名称に_の使用は禁止です\")\n",
        "\n",
        "\n",
        "validate_dataset_name(DATASET_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrbRPEvvisG0"
      },
      "source": [
        "# install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KxXWNwhisG0"
      },
      "outputs": [],
      "source": [
        "if ENV_NAME != \"GCP\":\n",
        "    %pip install -qq polars==1.0.0\n",
        "    %pip install -qq transformers==4.42.3\n",
        "    %pip install -qq datasets==2.20.0\n",
        "    %pip install -qq evaluate==0.4.2\n",
        "    %pip install -qq seqeval==1.2.2\n",
        "    %pip install -qq accelerate==0.32.0\n",
        "    %pip install -qq python-dotenv==1.0.1\n",
        "    %pip install -qq wandb==0.17.4\n",
        "    %pip install -qq bitsandbytes==0.43.1\n",
        "    %pip install -qq accelerate==0.32.0\n",
        "    %pip install -qq peft==0.11.1\n",
        "\n",
        "    # formatter\n",
        "    %pip install -qq black isort\n",
        "\n",
        "    %pip install -qq kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3uVqXs-isG1"
      },
      "source": [
        "# import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPumFSeNisG1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import ast\n",
        "import json\n",
        "\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, LayerNorm, MSELoss\n",
        "import wandb\n",
        "from datasets import (\n",
        "    Dataset,\n",
        "    DatasetDict,\n",
        "    Value,\n",
        "    concatenate_datasets,\n",
        "    load_dataset,\n",
        "    ClassLabel,\n",
        ")\n",
        "from tokenizers import AddedToken\n",
        "from tqdm.auto import tqdm\n",
        "from scipy.special import softmax\n",
        "from sklearn.metrics import log_loss\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModel,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig,\n",
        "    Gemma2ForSequenceClassification,\n",
        "    GemmaTokenizerFast,\n",
        "    Gemma2Config,\n",
        "    PreTrainedTokenizerBase,\n",
        "    EvalPrediction,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "\n",
        "from sklearn.metrics import log_loss, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLkGFO2PisG1"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
        "NUM_PROC = os.cpu_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ahs0mPMisG1"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import datasets\n",
        "import evaluate\n",
        "import bitsandbytes\n",
        "import accelerate\n",
        "import peft\n",
        "\n",
        "assert transformers.__version__ == \"4.42.3\"\n",
        "assert datasets.__version__ == \"2.20.0\"\n",
        "assert evaluate.__version__ == \"0.4.2\"\n",
        "assert bitsandbytes.__version__ == \"0.43.1\"\n",
        "assert accelerate.__version__ == \"0.32.0\"\n",
        "assert peft.__version__ == \"0.11.1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3ZVc72wisG2"
      },
      "outputs": [],
      "source": [
        "# Seed the same seed to all\n",
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-7WkHGUisG2",
        "outputId": "496ffc33-f794-43fa-fe5c-b9bc1d683e00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(f\"{DATA_PATH}/.env\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohXfhbqXisG2"
      },
      "source": [
        "# Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761,
          "referenced_widgets": [
            "8c1eeb7c07554b1e8e3901e46bc93937"
          ]
        },
        "id": "KGR9aE2YisG2",
        "outputId": "7070ca39-f06c-4996-9569-094a8a96bd7d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:kxkjq60y) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c1eeb7c07554b1e8e3901e46bc93937",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▆▄▄▂▅▄▅▂▅▂▄▄▃▂▃▄▃▂▂▁▂▃▄▃▃▄▂▃▂▃▁▂▄▁▂▂▂▂▃</td></tr><tr><td>train/learning_rate</td><td>▁▂▃▄▅▆▇███████████▇▇▇▇▇▇▇▆▆▆▆▆▆▆▅▅▅▅▄▄▄▄</td></tr><tr><td>train/loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▁▂▃▂▂▁▂▁▂▂▁▁▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>0.58344</td></tr><tr><td>train/global_step</td><td>262</td></tr><tr><td>train/grad_norm</td><td>1.73247</td></tr><tr><td>train/learning_rate</td><td>9e-05</td></tr><tr><td>train/loss</td><td>0.9227</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">e042-full-fine-tune</strong> at: <a href='https://wandb.ai/sinchir0/lmsys/runs/kxkjq60y' target=\"_blank\">https://wandb.ai/sinchir0/lmsys/runs/kxkjq60y</a><br/> View project at: <a href='https://wandb.ai/sinchir0/lmsys' target=\"_blank\">https://wandb.ai/sinchir0/lmsys</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240722_003629-kxkjq60y/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:kxkjq60y). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.17.5 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240722_095746-b84ap6lv</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sinchir0/lmsys/runs/b84ap6lv' target=\"_blank\">e046-full-fine-tune-1536</a></strong> to <a href='https://wandb.ai/sinchir0/lmsys' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/sinchir0/lmsys' target=\"_blank\">https://wandb.ai/sinchir0/lmsys</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/sinchir0/lmsys/runs/b84ap6lv' target=\"_blank\">https://wandb.ai/sinchir0/lmsys/runs/b84ap6lv</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'wandb'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if WANDB:\n",
        "    wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n",
        "    wandb.init(project=COMPETITION_NAME, name=EXP_NAME)\n",
        "    REPORT_TO = \"wandb\"\n",
        "else:\n",
        "    REPORT_TO = \"none\"\n",
        "\n",
        "REPORT_TO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxyhkSG4isG2"
      },
      "source": [
        "# Data Import & Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6X0GBaE-isG2"
      },
      "outputs": [],
      "source": [
        "with open(f\"{DATA_PATH}/label_stratified_fold.json\") as f:\n",
        "    label_stratified_fold = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlTiKm6TisG2"
      },
      "outputs": [],
      "source": [
        "train = (\n",
        "    pl.read_csv(f\"{DATA_PATH}/train.csv\")\n",
        "    # .with_columns(\n",
        "    #     pl.col(\"prompt\").str.json_decode(),\n",
        "    #     pl.col(\"response_a\").str.json_decode(),\n",
        "    #     pl.col(\"response_b\").str.json_decode(),\n",
        "    # )\n",
        "    # .with_columns(  # 長さの情報を追加する\n",
        "    #     pl.col(\"prompt\")\n",
        "    #     .map_elements(lambda x: len(x), return_dtype=pl.Int64)\n",
        "    #     .alias(\"len_prompt\"),\n",
        "    #     pl.col(\"response_a\")\n",
        "    #     .map_elements(lambda x: len(x), return_dtype=pl.Int64)\n",
        "    #     .alias(\"len_response_a\"),\n",
        "    #     pl.col(\"response_b\")\n",
        "    #     .map_elements(lambda x: len(x), return_dtype=pl.Int64)\n",
        "    #     .alias(\"len_response_b\"),\n",
        "    # )\n",
        "    # .with_columns(  # 最後のレスポンスのみを取得する\n",
        "    #     pl.col(\"prompt\")\n",
        "    #     .map_elements(lambda x: x[-1], return_dtype=pl.String)\n",
        "    #     .alias(\"last_prompt\"),\n",
        "    #     pl.col(\"response_a\")\n",
        "    #     .map_elements(lambda x: x[-1], return_dtype=pl.String)\n",
        "    #     .alias(\"last_response_a\"),\n",
        "    #     pl.col(\"response_b\")\n",
        "    #     .map_elements(lambda x: x[-1], return_dtype=pl.String)\n",
        "    #     .alias(\"last_response_b\"),\n",
        "    # )\n",
        "    # .with_columns(  # 最後のレスポンスがNoneの場合を空文字にする、約60件程度\n",
        "    #     pl.col(\"last_response_a\").fill_null(\"\"),\n",
        "    #     pl.col(\"last_response_b\").fill_null(\"\"),\n",
        "    # )\n",
        "    # .with_columns(  # labelを付与する\n",
        "    #     pl.when(pl.col(\"winner_model_a\") == 1)\n",
        "    #     .then(0)\n",
        "    #     .when(pl.col(\"winner_model_b\") == 1)\n",
        "    #     .then(1)\n",
        "    #     .when(pl.col(\"winner_tie\") == 1)\n",
        "    #     .then(2)\n",
        "    #     .alias(\"label\"),\n",
        "    # )\n",
        "    # .select(  # 元のprompt, responseを削除する\n",
        "    #     pl.exclude([\"prompt\", \"response_a\", \"response_b\"])\n",
        "    # )\n",
        "    .with_columns(  # foldを追加する\n",
        "        pl.col(\"id\").replace(label_stratified_fold).alias(\"fold\")\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nkbuZVjisG3"
      },
      "outputs": [],
      "source": [
        "if DEBUG:\n",
        "    train = train.head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTg7of_kisG3"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset.from_polars(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd10PoSKisG3",
        "outputId": "6bc87bd5-c1d1-43aa-d7a2-91ecf1253984"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie', 'fold'],\n",
              "    num_rows: 57477\n",
              "})"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFYU2E04isG3"
      },
      "outputs": [],
      "source": [
        "# 計算を早くするために、データを減らす\n",
        "if not DEBUG:\n",
        "    train_dataset = train_dataset.shuffle().select(\n",
        "        range(\n",
        "            int(len(train_dataset) * USE_DATA_RATE)\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHoxXNPjisG3"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q280EkU0isG3"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA, # low-rankマトリクスのスケーリングファクター\n",
        "    # only target self-attention\n",
        "    # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
        "    target_modules=[ # Linear層を全て含める\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\"\n",
        "    ],\n",
        "    layers_to_transform=[i for i in range(42) if i >= FREEZE_LAYERS],\n",
        "    lora_dropout=LORA_DROPOUT, # LoRAレイヤーのドロップアウト確率\n",
        "    bias=LORA_BIAS,\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elrKuvnhisG3"
      },
      "outputs": [],
      "source": [
        "# tokenizer = GemmaTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.add_eos_token = True  # We'll add <eos> at the end\n",
        "tokenizer.padding_side = \"right\"\n",
        "# tokenizer.add_special_tokens({\"additional_special_tokens\": [\"[SEP]\"]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jySgfPOTisG3",
        "outputId": "69b2bf11-4e38-40ed-8927-e8932b801f91"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at unsloth/gemma-2-9b-it-bnb-4bit and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PeftModelForSequenceClassification(\n",
            "  (base_model): LoraModel(\n",
            "    (model): Gemma2ForSequenceClassification(\n",
            "      (model): Gemma2Model(\n",
            "        (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
            "        (layers): ModuleList(\n",
            "          (0-41): 42 x Gemma2DecoderLayer(\n",
            "            (self_attn): Gemma2SdpaAttention(\n",
            "              (q_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=3584, out_features=4096, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (k_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (v_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (o_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=3584, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (rotary_emb): Gemma2RotaryEmbedding()\n",
            "            )\n",
            "            (mlp): Gemma2MLP(\n",
            "              (gate_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (up_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (down_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=14336, out_features=3584, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (act_fn): PytorchGELUTanh()\n",
            "            )\n",
            "            (input_layernorm): Gemma2RMSNorm()\n",
            "            (post_attention_layernorm): Gemma2RMSNorm()\n",
            "            (pre_feedforward_layernorm): Gemma2RMSNorm()\n",
            "            (post_feedforward_layernorm): Gemma2RMSNorm()\n",
            "          )\n",
            "        )\n",
            "        (norm): Gemma2RMSNorm()\n",
            "      )\n",
            "      (score): ModulesToSaveWrapper(\n",
            "        (original_module): Linear(in_features=3584, out_features=3, bias=False)\n",
            "        (modules_to_save): ModuleDict(\n",
            "          (default): Linear(in_features=3584, out_features=3, bias=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "trainable params: 54,028,800 || all params: 9,295,745,536 || trainable%: 0.5812\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=NUM_LABELS,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    cache_dir=\"./model_cache\"\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_config)\n",
        "# model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=16)\n",
        "print(model)\n",
        "print(model.print_trainable_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-7dZwvZisG4"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO7YiwQeisG4"
      },
      "source": [
        "# Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQT3oLTVisG4"
      },
      "outputs": [],
      "source": [
        "class CustomTokenizer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: PreTrainedTokenizerBase,\n",
        "        max_length: int\n",
        "    ) -> None:\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __call__(self, batch: dict) -> dict:\n",
        "        prompt = [\"<prompt>: \" + self.process_text(t) for t in batch[\"prompt\"]]\n",
        "        response_a = [\"\\n\\n<response_a>: \" + self.process_text(t) for t in batch[\"response_a\"]]\n",
        "        response_b = [\"\\n\\n<response_b>: \" + self.process_text(t) for t in batch[\"response_b\"]]\n",
        "        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
        "        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)\n",
        "        labels=[]\n",
        "        for a_win, b_win in zip(batch[\"winner_model_a\"], batch[\"winner_model_b\"]):\n",
        "            if a_win:\n",
        "                label = 0\n",
        "            elif b_win:\n",
        "                label = 1\n",
        "            else:\n",
        "                label = 2\n",
        "            labels.append(label)\n",
        "        return {**tokenized, \"labels\": labels}\n",
        "\n",
        "    @staticmethod\n",
        "    def process_text(text: str) -> str:\n",
        "        return \" \".join(eval(text, {\"null\": \"\"}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "42643422600c4396ab6c321c04926e8f",
            "a44dce271dde46998185ed728c5161a9",
            "c6ef84ad80c74741a497c94732266804",
            "1738427a42ae43b393dc79047bb9cb3e",
            "e40a05cf544a4a4ab354611c169349c4",
            "5321a39a335f43cbaf0c7dfb4524dcf4",
            "2788534ef4b44f218871963279c7d2b2",
            "5735fbe35e964a89879bb21385cf0544",
            "39b64c1b893a4000927d96ef71d9fc36",
            "49744d31631549fca7a0ff2424296ba3",
            "8cf7f583e70c49e8b46d09db5c4d35c4"
          ]
        },
        "id": "NshkFfjWisG4",
        "outputId": "b5de2f3b-8619-42f7-c757-8f8e7107d938"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42643422600c4396ab6c321c04926e8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/57477 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_dataset = train_dataset.map(\n",
        "    CustomTokenizer(tokenizer, max_length=TRAINING_MAX_LENGTH),\n",
        "    batched=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_EG6sLQisG4",
        "outputId": "2060c08c-e0b0-4a5c-c11b-e8cb422e67c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie', 'fold', 'input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 57477\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxZYjUbgisG5"
      },
      "outputs": [],
      "source": [
        "# def tokenize(examples, max_token_length: int):\n",
        "#     separator = \" [SEP] \"\n",
        "\n",
        "#     joined_text = (\n",
        "#         examples[\"last_prompt\"]\n",
        "#         + separator\n",
        "#         + examples[\"last_response_a\"]\n",
        "#         + separator\n",
        "#         + examples[\"last_response_b\"]\n",
        "#     )\n",
        "\n",
        "#     return tokenizer(\n",
        "#         joined_text,\n",
        "#         max_length=max_token_length,\n",
        "#         truncation=True,\n",
        "#         padding=\"max_length\",\n",
        "#     )\n",
        "\n",
        "\n",
        "# train_dataset = train_dataset.map(\n",
        "#     tokenize,\n",
        "#     batched=False,\n",
        "#     fn_kwargs={\"max_token_length\": TRAINING_MAX_LENGTH},\n",
        "#     num_proc=NUM_PROC,\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb_WWMPkisG5"
      },
      "source": [
        "# Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f86H6hZGisG5"
      },
      "outputs": [],
      "source": [
        "# filtered_train = train_dataset.filter(lambda x: x[\"fold\"] != USE_FOLD, num_proc=NUM_PROC)\n",
        "# filtered_valid = train_dataset.filter(lambda x: x[\"fold\"] == USE_FOLD, num_proc=NUM_PROC)\n",
        "# filtered_valid = filtered_valid.select(range(min(VALID_DATA_SIZE, len(filtered_valid))))\n",
        "\n",
        "# train_valid_dataset = DatasetDict(\n",
        "#     {\n",
        "#         \"train\": filtered_train,\n",
        "#         \"valid\": filtered_valid,\n",
        "#     }\n",
        "# )\n",
        "\n",
        "# del filtered_train, filtered_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKqwVX_risG5",
        "outputId": "c7af8472-49b8-41ee-d222-7e6ae331b969"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie', 'fold', 'input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 57477\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyY1QXduisG5"
      },
      "outputs": [],
      "source": [
        "# def compute_metrics(eval_pred):\n",
        "#     predictions, labels = eval_pred\n",
        "#     preds_prob = softmax(predictions, axis=-1)\n",
        "#     return {\"log_loss\": log_loss(labels, preds_prob)}\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred) -> dict:\n",
        "    preds, labels = eval_pred\n",
        "    preds_prob = softmax(preds, axis=-1)\n",
        "    return {\n",
        "        \"log_loss\": log_loss(y_true=labels, y_pred=preds_prob),\n",
        "        \"acc\": accuracy_score(y_true=labels, y_pred=preds.argmax(-1)),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfmFFtf0isG5"
      },
      "outputs": [],
      "source": [
        "# スケジューラの設定\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=MODEL_OUTPUT_PATH,\n",
        "    overwrite_output_dir=True,\n",
        "    learning_rate=LR,\n",
        "    per_device_train_batch_size=TRAIN_BS,\n",
        "    gradient_accumulation_steps=GRAD_ACC_STEP,\n",
        "    eval_accumulation_steps=GRAD_ACC_STEP,\n",
        "    # per_device_eval_batch_size=EVAL_BS,\n",
        "    num_train_epochs=EPOCH,\n",
        "    weight_decay=0.01,\n",
        "    do_eval=False,\n",
        "    # eval_strategy=\"steps\",\n",
        "    eval_strategy=\"no\",\n",
        "    # eval_steps=0.1,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=0.1,\n",
        "    save_total_limit=10,\n",
        "    logging_steps=2,\n",
        "    seed=SEED,\n",
        "    # metric_for_best_model=\"eval_loss\",\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine_with_restarts\", # \"linear\", # \"constant_with_warmup\",\n",
        "    report_to=REPORT_TO,\n",
        "    run_name=EXP_NAME,\n",
        "    # load_best_model_at_end=True,\n",
        "    fp16=True,\n",
        "    # fp16_full_eval=True,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"adamw_8bit\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    # train_dataset=train_valid_dataset[\"train\"],\n",
        "    train_dataset=train_dataset,\n",
        "    # train_dataset=ConcatDataset(train_valid_dataset[\"train\"]),\n",
        "    # eval_dataset=train_valid_dataset[\"valid\"],\n",
        "    # eval_dataset=ConcatDataset(train_valid_dataset[\"valid\"]),\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    # compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SBF60C8tisG5",
        "outputId": "2b3d7da8-fac7-4033-8c67-ff801bc53f7d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
            "\tsave_steps: 0.1 (from args) != 45 (from trainer_state.json)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='449' max='449' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [449/449 7:53:08, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>226</td>\n",
              "      <td>0.960900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.934900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.013400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>232</td>\n",
              "      <td>0.912900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>234</td>\n",
              "      <td>0.991800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>236</td>\n",
              "      <td>0.839500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>238</td>\n",
              "      <td>0.970100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.916000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>242</td>\n",
              "      <td>0.983700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>244</td>\n",
              "      <td>1.030000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>246</td>\n",
              "      <td>0.947100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>248</td>\n",
              "      <td>0.906100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.959900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>1.003700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>254</td>\n",
              "      <td>0.863000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>256</td>\n",
              "      <td>0.949300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>258</td>\n",
              "      <td>0.902000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>1.003400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>262</td>\n",
              "      <td>0.921200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>264</td>\n",
              "      <td>0.988900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>266</td>\n",
              "      <td>0.962200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>268</td>\n",
              "      <td>0.903300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.970800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>272</td>\n",
              "      <td>0.928300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>274</td>\n",
              "      <td>0.974900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>276</td>\n",
              "      <td>0.921900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>278</td>\n",
              "      <td>0.968900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.926300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>282</td>\n",
              "      <td>0.925000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>284</td>\n",
              "      <td>0.970100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>286</td>\n",
              "      <td>0.939300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>288</td>\n",
              "      <td>0.930900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.905600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>292</td>\n",
              "      <td>0.945400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>294</td>\n",
              "      <td>0.933500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>296</td>\n",
              "      <td>0.941900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>298</td>\n",
              "      <td>0.886200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.945600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>302</td>\n",
              "      <td>0.896100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>304</td>\n",
              "      <td>0.894600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>306</td>\n",
              "      <td>0.919400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>0.928400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.950100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>312</td>\n",
              "      <td>0.915100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>314</td>\n",
              "      <td>0.859700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>316</td>\n",
              "      <td>0.983800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>318</td>\n",
              "      <td>0.909800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.902400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>322</td>\n",
              "      <td>0.947800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>324</td>\n",
              "      <td>0.904600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>326</td>\n",
              "      <td>0.905000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>328</td>\n",
              "      <td>0.959200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.879400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>332</td>\n",
              "      <td>0.882900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>334</td>\n",
              "      <td>0.937300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>0.968200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>338</td>\n",
              "      <td>0.958600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.888500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>0.924000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>344</td>\n",
              "      <td>0.959400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>346</td>\n",
              "      <td>0.925500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>348</td>\n",
              "      <td>0.943700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.913500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>352</td>\n",
              "      <td>0.891200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>354</td>\n",
              "      <td>0.886800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>356</td>\n",
              "      <td>1.022000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>358</td>\n",
              "      <td>0.932200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.925900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>362</td>\n",
              "      <td>0.936000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>364</td>\n",
              "      <td>0.896800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>366</td>\n",
              "      <td>0.998400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>368</td>\n",
              "      <td>0.933700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.897100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>372</td>\n",
              "      <td>0.880900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>374</td>\n",
              "      <td>0.974400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>376</td>\n",
              "      <td>0.896900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>378</td>\n",
              "      <td>0.912400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.937000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>382</td>\n",
              "      <td>0.951700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>384</td>\n",
              "      <td>0.929400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>386</td>\n",
              "      <td>0.917900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>388</td>\n",
              "      <td>0.964600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.904800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>392</td>\n",
              "      <td>0.995300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>394</td>\n",
              "      <td>0.927300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>396</td>\n",
              "      <td>0.925300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>398</td>\n",
              "      <td>0.925400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.949300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>402</td>\n",
              "      <td>0.926600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>404</td>\n",
              "      <td>0.924900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>406</td>\n",
              "      <td>0.918400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>408</td>\n",
              "      <td>0.869000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.894900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>412</td>\n",
              "      <td>0.930500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>414</td>\n",
              "      <td>0.871700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>416</td>\n",
              "      <td>0.868100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>418</td>\n",
              "      <td>0.929400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.960900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>422</td>\n",
              "      <td>0.946400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>424</td>\n",
              "      <td>0.918800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>426</td>\n",
              "      <td>0.952100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>428</td>\n",
              "      <td>0.854500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.899400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>432</td>\n",
              "      <td>0.903500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>434</td>\n",
              "      <td>0.903400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>436</td>\n",
              "      <td>0.991500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>438</td>\n",
              "      <td>0.879400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.875000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>442</td>\n",
              "      <td>0.933800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>444</td>\n",
              "      <td>0.873700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>446</td>\n",
              "      <td>0.960900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>448</td>\n",
              "      <td>0.967600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "if TRAINING:\n",
        "    # モデルの学習\n",
        "    trainer.train(\n",
        "        resume_from_checkpoint = RESUME_FROM_CHECKPOINT if RESUME_FROM_CHECKPOINT else None\n",
        "    )\n",
        "    # ログの保存に利用したストレージを削除\n",
        "    # os.system(f\"rm -rf {MODEL_OUTPUT_PATH}/checkpoint-*\")\n",
        "    # モデルの保存\n",
        "    trainer.save_model(MODEL_OUTPUT_PATH)\n",
        "else:\n",
        "    # TRAINED_MODEL_PATHを用いて、学習済のモデルを読み込む\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        TRAINED_MODEL_PATH,\n",
        "        num_labels=NUM_LABELS,\n",
        "    )\n",
        "    # model = CustomDebertaSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        \".\",\n",
        "        per_device_eval_batch_size=4,\n",
        "        report_to=\"none\",\n",
        "        fp16=True,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=tokenizer,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDb8ehD7isG6"
      },
      "source": [
        "```\n",
        "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
        "TODO: この　Warningが問題ないのかを調べる\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6kx7STcisG6"
      },
      "source": [
        "# valid_datasetの作成・保存"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oSbl-eNpisG6"
      },
      "outputs": [],
      "source": [
        "# # TRAININGをINFERRENCEでMAX_TOKENを変えるために、validを作り直す\n",
        "# valid_dataset = train_dataset.filter(\n",
        "#     lambda example: example[\"id\"] in train_valid_dataset[\"valid\"][\"id\"],\n",
        "#     num_proc=NUM_PROC,\n",
        "# )\n",
        "\n",
        "# valid_dataset = valid_dataset.map(\n",
        "#     CustomTokenizer(tokenizer, max_length=INFERENCE_MAX_LENGTH),\n",
        "#     batched=True,\n",
        "#     num_proc=NUM_PROC,\n",
        "# )\n",
        "\n",
        "# # valid_dataset = valid_dataset.map(\n",
        "# #     tokenize,\n",
        "# #     batched=False,\n",
        "# #     fn_kwargs={\"max_token_length\": INFERENCE_MAX_LENGTH},\n",
        "# #     num_proc=NUM_PROC,\n",
        "# # )\n",
        "\n",
        "# # valid_dataset = valid_dataset.map(\n",
        "# #     tokenize,\n",
        "# #     batched=False,\n",
        "# #     fn_kwargs={\n",
        "# #         \"suffix\": \"a\",\n",
        "# #         \"max_token_length\": INFERENCE_MAX_LENGTH\n",
        "# #     },\n",
        "# #     num_proc=NUM_PROC,\n",
        "# # ).map(\n",
        "# #     tokenize,\n",
        "# #     batched=False,\n",
        "# #     fn_kwargs={\n",
        "# #         \"suffix\": \"b\",\n",
        "# #         \"max_token_length\": INFERENCE_MAX_LENGTH\n",
        "# #     },\n",
        "# #     num_proc=NUM_PROC,\n",
        "# # )\n",
        "\n",
        "\n",
        "# def add_valid_pred(example, idx, valid_pred):\n",
        "#     example[\"valid_pred\"] = valid_pred[idx]\n",
        "#     return example\n",
        "\n",
        "\n",
        "# valid_dataset = train_valid_dataset[\"valid\"]\n",
        "\n",
        "# valid_pred = softmax(trainer.predict(valid_dataset).predictions, axis=-1)\n",
        "# # valid_pred = softmax(trainer.predict(ConcatDataset(valid_dataset)).predictions, axis=-1)\n",
        "\n",
        "# np.save(f\"{MODEL_OUTPUT_PATH}/valid_prediction.npy\", valid_pred)\n",
        "\n",
        "# valid_dataset = valid_dataset.map(\n",
        "#     add_valid_pred, with_indices=True, fn_kwargs={\"valid_pred\": valid_pred}\n",
        "# )\n",
        "\n",
        "# valid_dataset.save_to_disk(f\"{MODEL_OUTPUT_PATH}/valid_dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDOqk7xeisG6"
      },
      "source": [
        "# CVの計算"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yFU5fHWw8es_"
      },
      "outputs": [],
      "source": [
        "# cv_score = log_loss(valid_dataset[\"labels\"], valid_pred)\n",
        "# print(f\"CV Score: {cv_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "exp87mDcisG7"
      },
      "outputs": [],
      "source": [
        "# # output_textを保存\n",
        "# with open(f\"{MODEL_OUTPUT_PATH}/cv_score.txt\", \"w\") as f:\n",
        "#     f.write(str(cv_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt4jBGyxisG7"
      },
      "source": [
        "# AWSへのアップロード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lupk0lEqisG7",
        "outputId": "5c5ed6a3-5451-4af1-f68c-1ce4672c9c75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/usr/bin/aws': No such file or directory\n",
            "rm: cannot remove '/usr/bin/aws_completer': No such file or directory\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 57.9M  100 57.9M    0     0   221M      0 --:--:-- --:--:-- --:--:--  221M\n",
            "You can now run: /usr/local/bin/aws --version\n"
          ]
        }
      ],
      "source": [
        "# S3へのアップロード\n",
        "# TODO: colabでは動かないため直す\n",
        "if not DEBUG and UPLOAD_DATA_TO_S3:\n",
        "    # uninstall\n",
        "    !sudo rm /usr/bin/aws\n",
        "    !sudo rm /usr/bin/aws_completer\n",
        "    !sudo rm -rf /usr/local/aws-cli\n",
        "\n",
        "    # install\n",
        "    !curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
        "    !unzip -o -qq awscliv2.zip\n",
        "    !sudo ./aws/install --update\n",
        "\n",
        "    # upload\n",
        "    output_name = MODEL_OUTPUT_PATH.split(\"/\")[-1]\n",
        "    os.system(\n",
        "        f\"aws s3 cp --recursive {MODEL_OUTPUT_PATH} s3://{COMPETITION_NAME}/trained_model/{output_name}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oA3GNoNAisG7"
      },
      "outputs": [],
      "source": [
        "# ダウンロード（参考）\n",
        "# !sudo rm /usr/bin/aws\n",
        "# !sudo rm /usr/bin/aws_completer\n",
        "# !sudo rm -rf /usr/local/aws-cli\n",
        "\n",
        "# !curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
        "# !unzip -o -qq awscliv2.zip\n",
        "# !sudo ./aws/install --update\n",
        "\n",
        "# !aws s3 cp --recursive s3://automated-essay-scoring/trained_model/e005-regression /notebooks/automated_essay_scoring/trained_models/e005-regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XyyX1JqisG7"
      },
      "source": [
        "# Kaggle Datasetへのupload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2yljmioJisG7",
        "outputId": "d6cae4bf-7704-4a91-cb5f-f596a83bcd53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.system(\"mkdir -p ~/.kaggle/\")\n",
        "os.system(f\"cp /{DATA_PATH}/kaggle.json ~/.kaggle/\")\n",
        "os.system(\"chmod 600 ~/.kaggle/kaggle.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EBXdM74XisG7",
        "outputId": "9a0bbfab-c412-479c-bbfc-7a4aaf192c8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create Dataset name:e046-full-fine-tune-1536-gemma-2-9b-it-bnb-4bit, output_dir:/content/drive/MyDrive/Kaggle/lmsys/trained_models/e046-full-fine-tune-1536\n",
            "Starting upload for file checkpoint-45.tar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 331M/331M [00:16<00:00, 20.7MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload successful: checkpoint-45.tar (331MB)\n",
            "Starting upload for file checkpoint-90.tar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 331M/331M [00:16<00:00, 20.4MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload successful: checkpoint-90.tar (331MB)\n",
            "Starting upload for file checkpoint-135.tar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 331M/331M [00:17<00:00, 20.0MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload successful: checkpoint-135.tar (331MB)\n",
            "Starting upload for file checkpoint-180.tar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 331M/331M [00:17<00:00, 20.0MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload successful: checkpoint-180.tar (331MB)\n",
            "Starting upload for file checkpoint-225.tar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 331M/331M [00:17<00:00, 20.1MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload successful: checkpoint-225.tar (331MB)\n",
            "Starting upload for file checkpoint-270.tar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 332M/332M [00:16<00:00, 20.6MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload successful: checkpoint-270.tar (332MB)\n",
            "Starting upload for file checkpoint-315.tar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 332M/332M [00:17<00:00, 20.0MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload successful: checkpoint-315.tar (332MB)\n",
            "Starting upload for file checkpoint-360.tar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 332M/332M [00:17<00:00, 19.8MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload successful: checkpoint-360.tar (332MB)\n",
            "Starting upload for file checkpoint-405.tar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 332M/332M [00:18<00:00, 19.3MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload successful: checkpoint-405.tar (332MB)\n",
            "Starting upload for file checkpoint-449.tar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 332M/332M [00:17<00:00, 20.5MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload successful: checkpoint-449.tar (332MB)\n",
            "Starting upload for file README.md\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.98k/4.98k [00:01<00:00, 3.34kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload successful: README.md (5KB)\n",
            "Starting upload for file adapter_model.safetensors\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 206M/206M [00:11<00:00, 19.0MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload successful: adapter_model.safetensors (206MB)\n",
            "Starting upload for file adapter_config.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.06k/1.06k [00:00<00:00, 1.46kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload successful: adapter_config.json (1KB)\n",
            "Starting upload for file tokenizer_config.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 39.7k/39.7k [00:01<00:00, 26.4kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload successful: tokenizer_config.json (40KB)\n",
            "Starting upload for file special_tokens_map.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 636/636 [00:01<00:00, 409B/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload successful: special_tokens_map.json (636B)\n",
            "Starting upload for file tokenizer.model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.04M/4.04M [00:02<00:00, 1.66MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload successful: tokenizer.model (4MB)\n",
            "Starting upload for file tokenizer.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16.7M/16.7M [00:03<00:00, 5.55MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload successful: tokenizer.json (17MB)\n",
            "Starting upload for file training_args.bin\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5.18k/5.18k [00:00<00:00, 5.78kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload successful: training_args.bin (5KB)\n"
          ]
        }
      ],
      "source": [
        "if not DEBUG and UPLOAD_DATA_TO_KAGGLE:\n",
        "    import os\n",
        "    import json\n",
        "\n",
        "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "    def dataset_create_new(dataset_name: str, upload_dir: str):\n",
        "        # if \"_\" in dataset_name:\n",
        "        #     raise ValueError(\"datasetの名称に_の使用は禁止です\")\n",
        "        dataset_metadata = {}\n",
        "        dataset_metadata[\"id\"] = f\"sinchir0/{dataset_name}\"\n",
        "        dataset_metadata[\"licenses\"] = [{\"name\": \"CC0-1.0\"}]\n",
        "        dataset_metadata[\"title\"] = dataset_name\n",
        "        with open(os.path.join(upload_dir, \"dataset-metadata.json\"), \"w\") as f:\n",
        "            json.dump(dataset_metadata, f, indent=4)\n",
        "        api = KaggleApi()\n",
        "        api.authenticate()\n",
        "        api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode=\"tar\")\n",
        "\n",
        "    print(f\"Create Dataset name:{DATASET_NAME}, output_dir:{MODEL_OUTPUT_PATH}\")\n",
        "    dataset_create_new(dataset_name=DATASET_NAME, upload_dir=MODEL_OUTPUT_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6KFg2LNisG8"
      },
      "source": [
        "# ローカルからのデータの削除"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tFhHkzi3isG8"
      },
      "outputs": [],
      "source": [
        "# if not DEBUG and REMOVE_LOCAL_FILE:\n",
        "#     # ローカルからは削除\n",
        "#     os.system(f\"rm -rf {MODEL_OUTPUT_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "7cea8d07b688472a889a0c03e39aa0d3"
          ]
        },
        "id": "bL2CHIBOisG8",
        "outputId": "53a88cb6-58d1-4149-fde3-172d3bcf770e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7cea8d07b688472a889a0c03e39aa0d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▅▄▃▅▂▂▅█▄▃▃▂▃▂▄▂▃▄▅▃▄▃▁▁▁▃▂▃▂▃▁▁▁▃▂▂▂▂▁▃</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▆█▁▇▄▂▇▇▆▄▄▅▅▅▃▅▇▅▆▅▆▆▄█▅▅▆▄▄▄▄▄▂▄▄▄▃▇▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>3.172621565143941e+18</td></tr><tr><td>train/epoch</td><td>0.99986</td></tr><tr><td>train/global_step</td><td>449</td></tr><tr><td>train/grad_norm</td><td>4.23141</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.9676</td></tr><tr><td>train_loss</td><td>0.46392</td></tr><tr><td>train_runtime</td><td>28522.9198</td></tr><tr><td>train_samples_per_second</td><td>2.015</td></tr><tr><td>train_steps_per_second</td><td>0.016</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">e046-full-fine-tune-1536</strong> at: <a href='https://wandb.ai/sinchir0/lmsys/runs/b84ap6lv' target=\"_blank\">https://wandb.ai/sinchir0/lmsys/runs/b84ap6lv</a><br/> View project at: <a href='https://wandb.ai/sinchir0/lmsys' target=\"_blank\">https://wandb.ai/sinchir0/lmsys</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240722_095746-b84ap6lv/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if WANDB:\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ohN9BBugisG8",
        "outputId": "6fca1d22-0fb4-43fa-ff03-e30bd49eb713"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "finish Notebook!\n"
          ]
        }
      ],
      "source": [
        "print(\"finish Notebook!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mH-uPM5EuoqS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1738427a42ae43b393dc79047bb9cb3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49744d31631549fca7a0ff2424296ba3",
            "placeholder": "​",
            "style": "IPY_MODEL_8cf7f583e70c49e8b46d09db5c4d35c4",
            "value": " 57477/57477 [00:35&lt;00:00, 1642.96 examples/s]"
          }
        },
        "2788534ef4b44f218871963279c7d2b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39b64c1b893a4000927d96ef71d9fc36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42643422600c4396ab6c321c04926e8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a44dce271dde46998185ed728c5161a9",
              "IPY_MODEL_c6ef84ad80c74741a497c94732266804",
              "IPY_MODEL_1738427a42ae43b393dc79047bb9cb3e"
            ],
            "layout": "IPY_MODEL_e40a05cf544a4a4ab354611c169349c4"
          }
        },
        "49744d31631549fca7a0ff2424296ba3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5321a39a335f43cbaf0c7dfb4524dcf4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5735fbe35e964a89879bb21385cf0544": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cf7f583e70c49e8b46d09db5c4d35c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a44dce271dde46998185ed728c5161a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5321a39a335f43cbaf0c7dfb4524dcf4",
            "placeholder": "​",
            "style": "IPY_MODEL_2788534ef4b44f218871963279c7d2b2",
            "value": "Map: 100%"
          }
        },
        "c6ef84ad80c74741a497c94732266804": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5735fbe35e964a89879bb21385cf0544",
            "max": 57477,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39b64c1b893a4000927d96ef71d9fc36",
            "value": 57477
          }
        },
        "e40a05cf544a4a4ab354611c169349c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}