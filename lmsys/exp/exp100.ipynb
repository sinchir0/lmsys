{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm==4.4.0 in /opt/conda/lib/python3.10/site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from lightgbm==4.4.0) (1.25.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from lightgbm==4.4.0) (1.11.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm==4.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import polars as pl\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_from_disk(\"../../trained_models/e027-make-valid/valid_dataset\").to_polars()\n",
    "pred = np.load(\"../../trained_models/e027-make-valid/valid_prediction.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pl.concat([data.select(pl.exclude(\"valid_pred\")), pl.DataFrame(pred, schema=[\"winner_a\", \"winner_b\", \"tie\"])], how=\"horizontal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (19_159, 20)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>model_a</th><th>model_b</th><th>prompt</th><th>response_a</th><th>response_b</th><th>winner_model_a</th><th>winner_model_b</th><th>winner_tie</th><th>fold</th><th>input_ids</th><th>attention_mask</th><th>labels</th><th>winner_a</th><th>winner_b</th><th>tie</th><th>turn_num</th><th>prompt_word_num</th><th>response_a_word_num</th><th>response_b_word_num</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>list[i32]</td><td>list[i8]</td><td>i64</td><td>f32</td><td>f32</td><td>f32</td><td>i32</td><td>i32</td><td>i32</td><td>i32</td></tr></thead><tbody><tr><td>2653302469</td><td>&quot;vicuna-7b&quot;</td><td>&quot;llama-2-13b-chat&quot;</td><td>&quot;[&quot;can you spot the clever pun …</td><td>&quot;[&quot;The nickname \\&quot;typlo\\&quot; conta…</td><td>&quot;[&quot;Sure, I&#x27;d be happy to help! …</td><td>0</td><td>0</td><td>1</td><td>2</td><td>[2, 235322, … 1]</td><td>[1, 1, … 1]</td><td>2</td><td>0.209556</td><td>0.206912</td><td>0.583532</td><td>1</td><td>52</td><td>334</td><td>307</td></tr><tr><td>3366614613</td><td>&quot;claude-2.1&quot;</td><td>&quot;mistral-medium&quot;</td><td>&quot;[&quot;What other features can we a…</td><td>&quot;[&quot;Here are some additional fea…</td><td>&quot;[&quot;Here are some additional fea…</td><td>0</td><td>1</td><td>0</td><td>2</td><td>[2, 235322, … 1]</td><td>[1, 1, … 1]</td><td>1</td><td>0.311937</td><td>0.23917</td><td>0.448893</td><td>9</td><td>17995</td><td>11321</td><td>21616</td></tr><tr><td>4229518975</td><td>&quot;vicuna-33b&quot;</td><td>&quot;llama-2-70b-chat&quot;</td><td>&quot;[&quot;Sally (a girl) has 6 brother…</td><td>&quot;[&quot;Sally has 3 sisters.&quot;]&quot;</td><td>&quot;[&quot;Each of Sally&#x27;s brothers has…</td><td>0</td><td>0</td><td>1</td><td>2</td><td>[2, 235322, … 1]</td><td>[1, 1, … 1]</td><td>2</td><td>0.138966</td><td>0.068726</td><td>0.792308</td><td>1</td><td>92</td><td>20</td><td>117</td></tr><tr><td>95091053</td><td>&quot;claude-2.1&quot;</td><td>&quot;yi-34b-chat&quot;</td><td>&quot;[&quot;hello how are you&quot;]&quot;</td><td>&quot;[&quot;I&#x27;m doing well, thanks for a…</td><td>&quot;[&quot;Thank you for asking! As an …</td><td>1</td><td>0</td><td>0</td><td>2</td><td>[2, 235322, … 1]</td><td>[1, 1, … 1]</td><td>0</td><td>0.569409</td><td>0.252705</td><td>0.177887</td><td>1</td><td>17</td><td>34</td><td>155</td></tr><tr><td>437433165</td><td>&quot;wizardlm-13b&quot;</td><td>&quot;koala-13b&quot;</td><td>&quot;[&quot;Explain the phrase &#x27;kill two…</td><td>&quot;[&quot;The phrase \\&quot;kill two birds …</td><td>&quot;[&quot;\\&quot;Kill two birds with one st…</td><td>0</td><td>0</td><td>1</td><td>2</td><td>[2, 235322, … 1]</td><td>[1, 1, … 1]</td><td>2</td><td>0.364599</td><td>0.281328</td><td>0.354073</td><td>1</td><td>74</td><td>481</td><td>766</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>3943120934</td><td>&quot;openchat-3.5&quot;</td><td>&quot;mistral-medium&quot;</td><td>&quot;[&quot;Why has France greatly de-in…</td><td>&quot;[&quot;France has not greatly de-in…</td><td>&quot;[&quot;It&#x27;s important to note that …</td><td>0</td><td>1</td><td>0</td><td>2</td><td>[2, 235322, … 1]</td><td>[1, 1, … 1]</td><td>1</td><td>0.131563</td><td>0.62681</td><td>0.241627</td><td>1</td><td>71</td><td>1846</td><td>1947</td></tr><tr><td>2289729730</td><td>&quot;vicuna-33b&quot;</td><td>&quot;guanaco-33b&quot;</td><td>&quot;[&quot;How many basic principles ac…</td><td>&quot;[&quot;Saltzer and Schroeder introd…</td><td>&quot;[&quot;Saltzer and Schroeder&#x27;s basi…</td><td>0</td><td>1</td><td>0</td><td>2</td><td>[2, 235322, … 1]</td><td>[1, 1, … 1]</td><td>1</td><td>0.182021</td><td>0.421766</td><td>0.396213</td><td>1</td><td>71</td><td>1310</td><td>1591</td></tr><tr><td>3023733933</td><td>&quot;mixtral-8x7b-instruct-v0.1&quot;</td><td>&quot;gpt-3.5-turbo-1106&quot;</td><td>&quot;[&quot;Write a paragraph about eati…</td><td>&quot;[&quot;In the most resplendent and …</td><td>&quot;[&quot;Verily, as the sun ascended …</td><td>1</td><td>0</td><td>0</td><td>2</td><td>[2, 235322, … 1]</td><td>[1, 1, … 1]</td><td>0</td><td>0.334755</td><td>0.438739</td><td>0.226506</td><td>1</td><td>129</td><td>1066</td><td>751</td></tr><tr><td>2886153616</td><td>&quot;llama-2-70b-chat&quot;</td><td>&quot;vicuna-33b&quot;</td><td>&quot;[&quot;what is the point of aerial …</td><td>&quot;[&quot;Aerial marker balls, also kn…</td><td>&quot;[&quot;Aerial marker balls, also kn…</td><td>1</td><td>0</td><td>0</td><td>2</td><td>[2, 235322, … 1]</td><td>[1, 1, … 1]</td><td>0</td><td>0.503419</td><td>0.140822</td><td>0.355759</td><td>2</td><td>133</td><td>4860</td><td>2870</td></tr><tr><td>1309929032</td><td>&quot;gpt-4-0613&quot;</td><td>&quot;llama-2-7b-chat&quot;</td><td>&quot;[&quot;Wich is faster, mod_php or p…</td><td>&quot;[&quot;In terms of performance, bot…</td><td>&quot;[&quot;Both mod_php and PHP-FPM are…</td><td>1</td><td>0</td><td>0</td><td>2</td><td>[2, 235322, … 1]</td><td>[1, 1, … 1]</td><td>0</td><td>0.234626</td><td>0.564491</td><td>0.200883</td><td>1</td><td>77</td><td>1235</td><td>2345</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (19_159, 20)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬──────────┬───────────┬───────────┬───────────┐\n",
       "│ id        ┆ model_a   ┆ model_b   ┆ prompt    ┆ … ┆ turn_num ┆ prompt_wo ┆ response_ ┆ response_ │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---      ┆ rd_num    ┆ a_word_nu ┆ b_word_nu │\n",
       "│ i64       ┆ str       ┆ str       ┆ str       ┆   ┆ i32      ┆ ---       ┆ m         ┆ m         │\n",
       "│           ┆           ┆           ┆           ┆   ┆          ┆ i32       ┆ ---       ┆ ---       │\n",
       "│           ┆           ┆           ┆           ┆   ┆          ┆           ┆ i32       ┆ i32       │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 265330246 ┆ vicuna-7b ┆ llama-2-1 ┆ [\"can you ┆ … ┆ 1        ┆ 52        ┆ 334       ┆ 307       │\n",
       "│ 9         ┆           ┆ 3b-chat   ┆ spot the  ┆   ┆          ┆           ┆           ┆           │\n",
       "│           ┆           ┆           ┆ clever    ┆   ┆          ┆           ┆           ┆           │\n",
       "│           ┆           ┆           ┆ pun …     ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 336661461 ┆ claude-2. ┆ mistral-m ┆ [\"What    ┆ … ┆ 9        ┆ 17995     ┆ 11321     ┆ 21616     │\n",
       "│ 3         ┆ 1         ┆ edium     ┆ other     ┆   ┆          ┆           ┆           ┆           │\n",
       "│           ┆           ┆           ┆ features  ┆   ┆          ┆           ┆           ┆           │\n",
       "│           ┆           ┆           ┆ can we a… ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 422951897 ┆ vicuna-33 ┆ llama-2-7 ┆ [\"Sally   ┆ … ┆ 1        ┆ 92        ┆ 20        ┆ 117       │\n",
       "│ 5         ┆ b         ┆ 0b-chat   ┆ (a girl)  ┆   ┆          ┆           ┆           ┆           │\n",
       "│           ┆           ┆           ┆ has 6     ┆   ┆          ┆           ┆           ┆           │\n",
       "│           ┆           ┆           ┆ brother…  ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 95091053  ┆ claude-2. ┆ yi-34b-ch ┆ [\"hello   ┆ … ┆ 1        ┆ 17        ┆ 34        ┆ 155       │\n",
       "│           ┆ 1         ┆ at        ┆ how are   ┆   ┆          ┆           ┆           ┆           │\n",
       "│           ┆           ┆           ┆ you\"]     ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 437433165 ┆ wizardlm- ┆ koala-13b ┆ [\"Explain ┆ … ┆ 1        ┆ 74        ┆ 481       ┆ 766       │\n",
       "│           ┆ 13b       ┆           ┆ the       ┆   ┆          ┆           ┆           ┆           │\n",
       "│           ┆           ┆           ┆ phrase    ┆   ┆          ┆           ┆           ┆           │\n",
       "│           ┆           ┆           ┆ 'kill     ┆   ┆          ┆           ┆           ┆           │\n",
       "│           ┆           ┆           ┆ two…      ┆   ┆          ┆           ┆           ┆           │\n",
       "│ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …        ┆ …         ┆ …         ┆ …         │\n",
       "│ 394312093 ┆ openchat- ┆ mistral-m ┆ [\"Why has ┆ … ┆ 1        ┆ 71        ┆ 1846      ┆ 1947      │\n",
       "│ 4         ┆ 3.5       ┆ edium     ┆ France    ┆   ┆          ┆           ┆           ┆           │\n",
       "│           ┆           ┆           ┆ greatly   ┆   ┆          ┆           ┆           ┆           │\n",
       "│           ┆           ┆           ┆ de-in…    ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 228972973 ┆ vicuna-33 ┆ guanaco-3 ┆ [\"How     ┆ … ┆ 1        ┆ 71        ┆ 1310      ┆ 1591      │\n",
       "│ 0         ┆ b         ┆ 3b        ┆ many      ┆   ┆          ┆           ┆           ┆           │\n",
       "│           ┆           ┆           ┆ basic pri ┆   ┆          ┆           ┆           ┆           │\n",
       "│           ┆           ┆           ┆ nciples   ┆   ┆          ┆           ┆           ┆           │\n",
       "│           ┆           ┆           ┆ ac…       ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 302373393 ┆ mixtral-8 ┆ gpt-3.5-t ┆ [\"Write a ┆ … ┆ 1        ┆ 129       ┆ 1066      ┆ 751       │\n",
       "│ 3         ┆ x7b-instr ┆ urbo-1106 ┆ paragraph ┆   ┆          ┆           ┆           ┆           │\n",
       "│           ┆ uct-v0.1  ┆           ┆ about     ┆   ┆          ┆           ┆           ┆           │\n",
       "│           ┆           ┆           ┆ eati…     ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 288615361 ┆ llama-2-7 ┆ vicuna-33 ┆ [\"what is ┆ … ┆ 2        ┆ 133       ┆ 4860      ┆ 2870      │\n",
       "│ 6         ┆ 0b-chat   ┆ b         ┆ the point ┆   ┆          ┆           ┆           ┆           │\n",
       "│           ┆           ┆           ┆ of aerial ┆   ┆          ┆           ┆           ┆           │\n",
       "│           ┆           ┆           ┆ …         ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 130992903 ┆ gpt-4-061 ┆ llama-2-7 ┆ [\"Wich is ┆ … ┆ 1        ┆ 77        ┆ 1235      ┆ 2345      │\n",
       "│ 2         ┆ 3         ┆ b-chat    ┆ faster,   ┆   ┆          ┆           ┆           ┆           │\n",
       "│           ┆           ┆           ┆ mod_php   ┆   ┆          ┆           ┆           ┆           │\n",
       "│           ┆           ┆           ┆ or p…     ┆   ┆          ┆           ┆           ┆           │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴──────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# feature engineering\n",
    "def to_list(text: str) -> list:\n",
    "    return eval(text, {\"null\": \"\"})\n",
    "\n",
    "def flatten(text_list: list) :\n",
    "    return list(itertools.chain.from_iterable(text_list))\n",
    "\n",
    "train = all_data.with_columns(\n",
    "    pl.col(\"prompt\").map_elements(lambda x : len(to_list(x)), return_dtype=pl.Int32).alias(\"turn_num\"),\n",
    "    pl.col(\"prompt\").map_elements(lambda x : len(flatten(to_list(x))), return_dtype=pl.Int32).alias(\"prompt_word_num\"),\n",
    "    pl.col(\"response_a\").map_elements(lambda x : len(flatten(to_list(x))), return_dtype=pl.Int32).alias(\"response_a_word_num\"),\n",
    "    pl.col(\"response_b\").map_elements(lambda x : len(flatten(to_list(x))), return_dtype=pl.Int32).alias(\"response_b_word_num\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_col = [\n",
    "    \"turn_num\",\n",
    "    \"prompt_word_num\",\n",
    "    \"response_a_word_num\",\n",
    "    \"response_b_word_num\"\n",
    "]\n",
    "\n",
    "TARGET_COL = \"labels\"\n",
    "N_FOLD = 3\n",
    "SEED = 42\n",
    "NUM_LABEL = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foldの作成\n",
    "fold_arr = np.zeros(train.height)\n",
    "fold = StratifiedKFold(n_splits=N_FOLD, shuffle=True, random_state=SEED)\n",
    "\n",
    "for idx, (_, val_idx) in enumerate(fold.split(train, train.get_column(TARGET_COL))):\n",
    "    fold_arr[val_idx] = idx\n",
    "\n",
    "train = train.with_columns(\n",
    "    pl.Series(fold_arr).cast(pl.Int64).alias(\"fold\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBMで用いるパラメータを指定\n",
    "params = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    'num_class': NUM_LABEL,\n",
    "    \"max_depth\": -1,\n",
    "    \"min_data_in_leaf\": 10,  # 1つの葉に入る最小のデータ数\n",
    "    \"num_leaves\": 24,  # 2**max_depthより少し小さめにすると過学習を防げる\n",
    "    \"learning_rate\": 0.01,  # 1回のiterationで学習を進める割合、大きいと学習が早く終わる。小さいと学習は長いが高精度になりやすい。\n",
    "    \"bagging_freq\": 5,  # 指定した回数ごとにbaggingを行う\n",
    "    \"feature_fraction\": 0.7,  # 1回のiterationで利用する特徴量(列方向)の割合\n",
    "    \"bagging_fraction\": 0.6,  # 1回のiterationで利用するデータ(行方向)の割合\n",
    "    \"verbose\": -1,  # 出力するログレベルの変更、-1はFatalなログのみを出力\n",
    "    \"seed\": SEED,  # ランダムシードの固定\n",
    "    \"lambda_l1\": 0.4,  # 正則化のためのパラメータ\n",
    "    \"lambda_l2\": 0.4,  # 正則化のためのパラメータ\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fold 0\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_logloss: 1.05216\tvalid_1's multi_logloss: 1.06596\n",
      "[200]\ttraining's multi_logloss: 1.03119\tvalid_1's multi_logloss: 1.05756\n",
      "[300]\ttraining's multi_logloss: 1.01683\tvalid_1's multi_logloss: 1.05462\n",
      "[400]\ttraining's multi_logloss: 1.00522\tvalid_1's multi_logloss: 1.05453\n",
      "Early stopping, best iteration is:\n",
      "[335]\ttraining's multi_logloss: 1.01255\tvalid_1's multi_logloss: 1.0542\n",
      "Start fold 1\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_logloss: 1.0484\tvalid_1's multi_logloss: 1.06901\n",
      "[200]\ttraining's multi_logloss: 1.02631\tvalid_1's multi_logloss: 1.06321\n",
      "[300]\ttraining's multi_logloss: 1.0114\tvalid_1's multi_logloss: 1.0625\n",
      "[400]\ttraining's multi_logloss: 0.999095\tvalid_1's multi_logloss: 1.06345\n",
      "Early stopping, best iteration is:\n",
      "[304]\ttraining's multi_logloss: 1.01091\tvalid_1's multi_logloss: 1.06245\n",
      "Start fold 2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_logloss: 1.04771\tvalid_1's multi_logloss: 1.0698\n",
      "[200]\ttraining's multi_logloss: 1.02473\tvalid_1's multi_logloss: 1.06473\n",
      "[300]\ttraining's multi_logloss: 1.0094\tvalid_1's multi_logloss: 1.06489\n",
      "Early stopping, best iteration is:\n",
      "[240]\ttraining's multi_logloss: 1.01814\tvalid_1's multi_logloss: 1.06453\n"
     ]
    }
   ],
   "source": [
    "# テストデータに対する推論、特徴量重要度(後述)を計算するために、モデルを保存するobjectを作成\n",
    "models = []\n",
    "\n",
    "# ① Cross Validationによる学習の実施\n",
    "for fold in range(N_FOLD):\n",
    "    print(f\"Start fold {fold}\")\n",
    "\n",
    "    # ② foldごとにtrainとvalidに分ける\n",
    "    train_fold = train.filter(pl.col(\"fold\") != fold)\n",
    "    valid_fold = train.filter(pl.col(\"fold\") == fold)\n",
    "\n",
    "    # ③ X(説明変数)とy(目的変数)に分ける\n",
    "    X_train = train_fold.select(use_col)\n",
    "    X_valid = valid_fold.select(use_col)\n",
    "    y_train = train_fold.select(TARGET_COL)\n",
    "    y_valid = valid_fold.select(TARGET_COL)\n",
    "\n",
    "    # ④ LightGBMが認識可能な形にデータセットを変換\n",
    "    # polars.DataFrame から pandas.DataFrame への変更を行っている\n",
    "    lgb_train = lgb.Dataset(X_train.to_pandas(), y_train.to_pandas())\n",
    "    lgb_eval = lgb.Dataset(\n",
    "        X_valid.to_pandas(), y_valid.to_pandas(), reference=lgb_train\n",
    "    )\n",
    "\n",
    "    # ⑤ モデルの学習\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        num_boost_round=10000,  # 学習のiteration回数\n",
    "        valid_sets=[lgb_train, lgb_eval],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(\n",
    "                stopping_rounds=100\n",
    "            ),  # Early stopingの回数、binary_loglossが改善しないiterationが100回続いたら学習を止める\n",
    "            lgb.log_evaluation(100),  # 指定したiteration回数ごとにlogを出力する\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # ⑥ モデルを保存\n",
    "    models.append([fold, model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
