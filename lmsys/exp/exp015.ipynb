{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目的\n",
    "largeでConcatを行う, bsを16にする, gradinet_acc_stepを修正する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path setting\n",
    "EXP_NAME = \"e015-use-concat-lrg\"\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "COMPETITION_NAME = \"lmsys\"\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "DATASET_NAME = f\"{EXP_NAME}-{MODEL_NAME.split('/')[-1]}\"\n",
    "MODEL_OUTPUT_PATH = f\"trained_models/{EXP_NAME}\"\n",
    "\n",
    "# experiment parameter\n",
    "DEBUG = False\n",
    "TRAINING = True\n",
    "UPLOAD_DATA_TO_S3 = True\n",
    "UPLOAD_DATA_TO_KAGGLE = True\n",
    "WANDB = True\n",
    "\n",
    "# model parameter\n",
    "TRAINING_MAX_LENGTH = 512\n",
    "INFERENCE_MAX_LENGTH = 1024 # 1536\n",
    "SEED = 42\n",
    "VALID_DATA_SIZE = 0.3\n",
    "EPOCH = 3\n",
    "LR = 2e-05\n",
    "TRAIN_BS = 16 # 32 # 8\n",
    "GRAD_ACC_NUM = 128 // TRAIN_BS\n",
    "EVAL_BS = 16 # 32 # 8\n",
    "NUM_LABELS = 3\n",
    "LABEL_SMOOTHING = 0.2\n",
    "\n",
    "RESUME_FROM_CHECKPOINT = True # 途中から再開する場合はTrueにする\n",
    "TRAINED_MODEL_PATH = \"lmsys/trained_models/e006-use-concat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 15 00:56:09 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 54%   66C    P8    60W / 300W |   2793MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.7\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/lmsys/lmsys/exp\n",
      "Jupyter Lab!\n",
      "../../data\n",
      "/notebooks/lmsys/lmsys/exp\n",
      "Jupyter Lab!\n",
      "../../trained_models/e015-use-concat-lrg\n"
     ]
    }
   ],
   "source": [
    "def resolve_path(base_path: str) -> str:\n",
    "    import os\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    print(cwd)\n",
    "    if cwd == f\"/notebooks\":\n",
    "        print(\"Jupyter Kernel By VSCode!\")\n",
    "        return f\"/notebooks/{COMPETITION_NAME}/{base_path}\"\n",
    "    elif cwd == f\"/notebooks/{COMPETITION_NAME}\":\n",
    "        print(\"nohup!\")\n",
    "        return base_path\n",
    "    elif cwd == f\"/notebooks/{COMPETITION_NAME}/{COMPETITION_NAME}/exp\":\n",
    "        print(\"Jupyter Lab!\")\n",
    "        return f\"../../{base_path}\"\n",
    "    else:\n",
    "        raise Exception(\"Unknown environment\")\n",
    "\n",
    "\n",
    "DATA_PATH = resolve_path(DATA_PATH)\n",
    "print(DATA_PATH)\n",
    "MODEL_OUTPUT_PATH = resolve_path(MODEL_OUTPUT_PATH)\n",
    "print(MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate_dataset_name(dataset_name: str) -> None:\n",
    "    if len(dataset_name) < 6 or len(dataset_name) > 50:\n",
    "        raise Exception(\n",
    "            f\"データセットの文字列は6~50文字にしてください。現在{len(DATASET_NAME)}文字\"\n",
    "        )\n",
    "    if \"_\" in dataset_name:\n",
    "        raise Exception(\"datasetの名称に_の使用は禁止です\")\n",
    "\n",
    "\n",
    "validate_dataset_name(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qq polars==1.0.0\n",
    "%pip install -qq transformers==4.42.3\n",
    "%pip install -qq datasets==2.20.0\n",
    "%pip install -qq evaluate==0.4.2\n",
    "%pip install -qq seqeval==1.2.2\n",
    "%pip install -qq accelerate==0.32.0\n",
    "%pip install -qq python-dotenv==1.0.1\n",
    "%pip install -qq wandb==0.17.4\n",
    "\n",
    "# formatter\n",
    "%pip install -qq black isort\n",
    "\n",
    "%pip install -qq kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-15 00:56:35.729141: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-15 00:56:35.729207: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-15 00:56:35.730233: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-15 00:56:35.736690: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-15 00:56:36.670939: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import ast\n",
    "import json\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, LayerNorm, MSELoss\n",
    "import wandb\n",
    "from datasets import Dataset, DatasetDict, Value, concatenate_datasets, load_dataset, ClassLabel\n",
    "from tokenizers import AddedToken\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import log_loss\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    DebertaV2PreTrainedModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.deberta_v2.modeling_deberta_v2 import (\n",
    "    ContextPooler,\n",
    "    StableDropout,\n",
    "    DebertaV2Model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "NUM_PROC = os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import evaluate\n",
    "\n",
    "assert transformers.__version__ == \"4.42.3\"\n",
    "assert datasets.__version__ == \"2.20.0\"\n",
    "assert evaluate.__version__ == \"0.4.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seed the same seed to all\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(f\"{DATA_PATH}/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msinchir0\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/lmsys/lmsys/exp/wandb/run-20240715_005641-q945dxsj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sinchir0/lmsys/runs/q945dxsj' target=\"_blank\">e015-use-concat-lrg</a></strong> to <a href='https://wandb.ai/sinchir0/lmsys' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sinchir0/lmsys' target=\"_blank\">https://wandb.ai/sinchir0/lmsys</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sinchir0/lmsys/runs/q945dxsj' target=\"_blank\">https://wandb.ai/sinchir0/lmsys/runs/q945dxsj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'wandb'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if WANDB:\n",
    "    wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n",
    "    wandb.init(project=COMPETITION_NAME, name=EXP_NAME)\n",
    "    REPORT_TO = \"wandb\"\n",
    "else:\n",
    "    REPORT_TO = \"none\"\n",
    "\n",
    "REPORT_TO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import & Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = (\n",
    "    pl.read_csv(f\"{DATA_PATH}/train.csv\")\n",
    "    .with_columns(\n",
    "        pl.col(\"prompt\").str.json_decode(),\n",
    "        pl.col(\"response_a\").str.json_decode(),\n",
    "        pl.col(\"response_b\").str.json_decode(),\n",
    "    )\n",
    "    .with_columns( # 長さの情報を追加する\n",
    "        pl.col(\"prompt\").map_elements(lambda x: len(x), return_dtype=pl.Int64).alias(\"len_prompt\"),\n",
    "        pl.col(\"response_a\").map_elements(lambda x: len(x), return_dtype=pl.Int64).alias(\"len_response_a\"),\n",
    "        pl.col(\"response_b\").map_elements(lambda x: len(x), return_dtype=pl.Int64).alias(\"len_response_b\"),\n",
    "    )\n",
    "    .with_columns( # 最後のレスポンスのみを取得する\n",
    "        pl.col(\"prompt\").map_elements(lambda x: x[-1], return_dtype=pl.String).alias(\"last_prompt\"),\n",
    "        pl.col(\"response_a\").map_elements(lambda x: x[-1], return_dtype=pl.String).alias(\"last_response_a\"),\n",
    "        pl.col(\"response_b\").map_elements(lambda x: x[-1], return_dtype=pl.String).alias(\"last_response_b\"),\n",
    "    )\n",
    "    .with_columns( # 最後のレスポンスがNoneの場合を空文字にする、約60件程度\n",
    "        pl.col(\"last_response_a\").fill_null(\"\"),\n",
    "        pl.col(\"last_response_b\").fill_null(\"\"),\n",
    "    )\n",
    "    .with_columns( # labelを付与する\n",
    "        pl.when(pl.col(\"winner_model_a\") == 1)\n",
    "        .then(0)\n",
    "        .when(pl.col(\"winner_model_b\") == 1)\n",
    "        .then(1)\n",
    "        .when(pl.col(\"winner_tie\") == 1)\n",
    "        .then(2)\n",
    "        .alias(\"label\"),\n",
    "    )\n",
    "    .select( # 元のprompt, responseを削除する\n",
    "        pl.exclude([\"prompt\", \"response_a\", \"response_b\"])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    train = train.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_polars(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie', 'len_prompt', 'len_response_a', 'len_response_b', 'last_prompt', 'last_response_a', 'last_response_b', 'label'],\n",
       "    num_rows: 57477\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://chatgpt.com/share/f7a30189-2ca6-4870-96e6-c06120bf4ca7\n",
    "\n",
    "# class CustomDebertaModel(nn.Module):\n",
    "#     def __init__(self, model_name):\n",
    "#         super(CustomDebertaModel, self).__init__()\n",
    "#         self.deberta = DebertaModel.from_pretrained(model_name)\n",
    "#         self.classification_head = nn.Sequential(\n",
    "#             nn.Linear(self.deberta.config.hidden_size * 2, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 1)  # Assuming binary classification\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input_ids_a, attention_mask_a, input_ids_b, attention_mask_b):\n",
    "#         outputs_a = self.deberta(input_ids=input_ids_a, attention_mask=attention_mask_a)\n",
    "#         outputs_b = self.deberta(input_ids=input_ids_b, attention_mask=attention_mask_b)\n",
    "        \n",
    "#         cls_a = outputs_a.last_hidden_state[:, 0, :]  # CLS token\n",
    "#         cls_b = outputs_b.last_hidden_state[:, 0, :]  # CLS token\n",
    "        \n",
    "#         combined = torch.cat((cls_a, cls_b), dim=1)\n",
    "#         logits = self.classification_head(combined)\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # https://github.com/sinchir0/automated_essay_scoring/blob/main/automated_essay_scoring/exp/exp030.ipynb\n",
    "\n",
    "# # https://dev.classmethod.jp/articles/huggingface-usage-custom-model/\n",
    "# # https://github.com/huggingface/transformers/blob/94b3f544a1f5e04b78d87a2ae32a7ac252e22e31/src/transformers/models/deberta_v2/modeling_deberta_v2.py#L1313\n",
    "# class CustomDebertaSequenceClassification(DebertaV2PreTrainedModel):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "\n",
    "#         num_labels = getattr(config, \"num_labels\", 2)\n",
    "#         self.num_labels = num_labels\n",
    "\n",
    "#         self.deberta = DebertaV2Model(config)\n",
    "#         self.pooler = ContextPooler(config)\n",
    "#         output_dim = self.pooler.output_dim\n",
    "\n",
    "#         self.classifier = nn.Linear(output_dim, num_labels)\n",
    "#         drop_out = getattr(config, \"cls_dropout\", None)\n",
    "#         drop_out = self.config.hidden_dropout_prob if drop_out is None else drop_out\n",
    "#         self.dropout = StableDropout(drop_out)\n",
    "\n",
    "#         # Initialize weights and apply final processing\n",
    "#         self.post_init()\n",
    "\n",
    "#     def get_input_embeddings(self):\n",
    "#         return self.deberta.get_input_embeddings()\n",
    "\n",
    "#     def set_input_embeddings(self, new_embeddings):\n",
    "#         self.deberta.set_input_embeddings(new_embeddings)\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         input_ids: Optional[torch.Tensor] = None,\n",
    "#         attention_mask: Optional[torch.Tensor] = None,\n",
    "#         token_type_ids: Optional[torch.Tensor] = None,\n",
    "#         # position_ids: Optional[torch.Tensor] = None,\n",
    "#         inputs_embeds: Optional[torch.Tensor] = None,\n",
    "#         labels: Optional[torch.Tensor] = None,\n",
    "#         # output_attentions: Optional[bool] = None,\n",
    "#         # output_hidden_states: Optional[bool] = None,\n",
    "#         return_dict: Optional[bool] = None,\n",
    "#     ) -> Union[Tuple, SequenceClassifierOutput]:\n",
    "#         return_dict = (\n",
    "#             return_dict if return_dict is not None else self.config.use_return_dict\n",
    "#         )\n",
    "\n",
    "#         outputs = self.deberta(\n",
    "#             input_ids,\n",
    "#             token_type_ids=token_type_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             # position_ids=position_ids,\n",
    "#             inputs_embeds=inputs_embeds,\n",
    "#             # output_attentions=output_attentions,\n",
    "#             # output_hidden_states=output_hidden_states,\n",
    "#             return_dict=return_dict,\n",
    "#         )\n",
    "\n",
    "#         encoder_layer = outputs[0]\n",
    "#         pooled_output = self.pooler(encoder_layer)\n",
    "#         pooled_output = self.dropout(pooled_output)\n",
    "#         logits = self.classifier(pooled_output)\n",
    "\n",
    "#         loss = None\n",
    "#         if labels is not None:\n",
    "#             loss_fn = nn.MSELoss()\n",
    "#             logits = logits.view(-1).to(labels.dtype)\n",
    "#             loss = loss_fn(logits, labels.view(-1))\n",
    "#         # if labels is not None:\n",
    "#         #     if self.config.problem_type is None:\n",
    "#         #         if self.num_labels == 1:\n",
    "#         #             # regression task\n",
    "#         #             loss_fn = nn.MSELoss()\n",
    "#         #             logits = logits.view(-1).to(labels.dtype)\n",
    "#         #             loss = loss_fn(logits, labels.view(-1))\n",
    "#         #         elif labels.dim() == 1 or labels.size(-1) == 1:\n",
    "#         #             label_index = (labels >= 0).nonzero()\n",
    "#         #             labels = labels.long()\n",
    "#         #             if label_index.size(0) > 0:\n",
    "#         #                 labeled_logits = torch.gather(\n",
    "#         #                     logits,\n",
    "#         #                     0,\n",
    "#         #                     label_index.expand(label_index.size(0), logits.size(1)),\n",
    "#         #                 )\n",
    "#         #                 labels = torch.gather(labels, 0, label_index.view(-1))\n",
    "#         #                 loss_fct = CrossEntropyLoss()\n",
    "#         #                 loss = loss_fct(\n",
    "#         #                     labeled_logits.view(-1, self.num_labels).float(),\n",
    "#         #                     labels.view(-1),\n",
    "#         #                 )\n",
    "#         #             else:\n",
    "#         #                 loss = torch.tensor(0).to(logits)\n",
    "#         #         else:\n",
    "#         #             log_softmax = nn.LogSoftmax(-1)\n",
    "#         #             loss = -((log_softmax(logits) * labels).sum(-1)).mean()\n",
    "#         #     elif self.config.problem_type == \"regression\":\n",
    "#         #         loss_fct = MSELoss()\n",
    "#         #         if self.num_labels == 1:\n",
    "#         #             loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "#         #         else:\n",
    "#         #             loss = loss_fct(logits, labels)\n",
    "#         #     elif self.config.problem_type == \"single_label_classification\":\n",
    "#         #         loss_fct = CrossEntropyLoss()\n",
    "#         #         loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "#         #     elif self.config.problem_type == \"multi_label_classification\":\n",
    "#         #         loss_fct = BCEWithLogitsLoss()\n",
    "#         #         loss = loss_fct(logits, labels)\n",
    "#         # if not return_dict:\n",
    "#         #     output = (logits,) + outputs[1:]\n",
    "#         #     return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "#         return SequenceClassifierOutput(\n",
    "#             loss=loss,\n",
    "#             logits=logits,\n",
    "#             # hidden_states=outputs.hidden_states,\n",
    "#             # attentions=outputs.attentions,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/sinchir0/automated_essay_scoring/blob/main/automated_essay_scoring/exp/exp030.ipynb\n",
    "\n",
    "# https://dev.classmethod.jp/articles/huggingface-usage-custom-model/\n",
    "# https://github.com/huggingface/transformers/blob/94b3f544a1f5e04b78d87a2ae32a7ac252e22e31/src/transformers/models/deberta_v2/modeling_deberta_v2.py#L1313\n",
    "class CustomDebertaSequenceClassification(DebertaV2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        #num_labels = getattr(config, \"num_labels\", 2)\n",
    "        # self.num_labels = num_labels\n",
    "        self.num_labels = NUM_LABELS\n",
    "\n",
    "        self.deberta = DebertaV2Model(config)\n",
    "        self.pooler = ContextPooler(config)\n",
    "        output_dim = self.pooler.output_dim\n",
    "\n",
    "        # self.classifier = nn.Linear(output_dim * 2, num_labels)\n",
    "        self.classifier = nn.Linear(output_dim * 2, NUM_LABELS)\n",
    "        drop_out = getattr(config, \"cls_dropout\", None)\n",
    "        drop_out = self.config.hidden_dropout_prob if drop_out is None else drop_out\n",
    "        self.dropout = StableDropout(drop_out)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.deberta.get_input_embeddings()\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings):\n",
    "        self.deberta.set_input_embeddings(new_embeddings)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids_a: torch.Tensor | None = None,\n",
    "        input_ids_b: torch.Tensor | None = None,\n",
    "        attention_mask_a: torch.Tensor | None = None,\n",
    "        attention_mask_b: torch.Tensor | None = None,\n",
    "        token_type_ids_a: torch.Tensor | None = None,\n",
    "        token_type_ids_b: torch.Tensor | None = None,\n",
    "        # position_ids: Optional[torch.Tensor] = None,\n",
    "        # inputs_embeds_a: torch.Tensor | None = None,\n",
    "        # inputs_embeds_b: torch.Tensor | None = None,\n",
    "        labels: torch.Tensor | None = None,\n",
    "        # output_attentions: Optional[bool] = None,\n",
    "        # output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: bool | None = None,\n",
    "    ) -> tuple | SequenceClassifierOutput:\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        )\n",
    "\n",
    "        outputs_a = self.deberta(\n",
    "            input_ids_a,\n",
    "            token_type_ids=token_type_ids_a,\n",
    "            attention_mask=attention_mask_a,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        outputs_b = self.deberta(\n",
    "            input_ids_b,\n",
    "            token_type_ids=token_type_ids_b,\n",
    "            attention_mask=attention_mask_b,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        encoder_layer_a = outputs_a[0]\n",
    "        encoder_layer_b = outputs_b[0]\n",
    "        \n",
    "        pooled_output_a = self.pooler(encoder_layer_a)\n",
    "        pooled_output_b = self.pooler(encoder_layer_b)\n",
    "\n",
    "        concated_pooled_output = torch.cat((pooled_output_a, pooled_output_b), dim=1)\n",
    "        \n",
    "        logits = self.classifier(concated_pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        \n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    # regression task\n",
    "                    loss_fn = nn.MSELoss()\n",
    "                    logits = logits.view(-1).to(labels.dtype)\n",
    "                    loss = loss_fn(logits, labels.view(-1))\n",
    "                elif labels.dim() == 1 or labels.size(-1) == 1:\n",
    "                    label_index = (labels >= 0).nonzero()\n",
    "                    labels = labels.long()\n",
    "                    if label_index.size(0) > 0:\n",
    "                        labeled_logits = torch.gather(\n",
    "                            logits,\n",
    "                            0,\n",
    "                            label_index.expand(label_index.size(0), logits.size(1)),\n",
    "                        )\n",
    "                        labels = torch.gather(labels, 0, label_index.view(-1))\n",
    "                        loss_fct = CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "                        loss = loss_fct(\n",
    "                            labeled_logits.view(-1, self.num_labels).float(),\n",
    "                            labels.view(-1),\n",
    "                        )\n",
    "                    else:\n",
    "                        loss = torch.tensor(0).to(logits)\n",
    "                else:\n",
    "                    log_softmax = nn.LogSoftmax(-1)\n",
    "                    loss = -((log_softmax(logits) * labels).sum(-1)).mean()\n",
    "            elif self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        # if not return_dict:\n",
    "        #     output = (logits,) + outputs[1:]\n",
    "        #     return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            # hidden_states=outputs.hidden_states,\n",
    "            # attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:562: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of CustomDebertaSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(128016, 1024)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME\n",
    ")\n",
    "tokenizer.add_tokens(\n",
    "    [\n",
    "        AddedToken(\"\\n\", normalized=False),\n",
    "        AddedToken(\" \" * 2, normalized=False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "class CustomDataCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=16)\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch_a = [{'input_ids': f['input_ids_a'], 'token_type_ids': f['token_type_ids_a'], 'attention_mask': f['attention_mask_a']} for f in features]\n",
    "        batch_b = [{'input_ids': f['input_ids_b'], 'token_type_ids': f['token_type_ids_b'], 'attention_mask': f['attention_mask_b']} for f in features]\n",
    "        \n",
    "        batch_a = self.data_collator(batch_a)\n",
    "        batch_b = self.data_collator(batch_b)\n",
    "        \n",
    "        labels = torch.tensor([f['labels'] for f in features])\n",
    "\n",
    "        return {\n",
    "            'input_ids_a': batch_a['input_ids'],\n",
    "            'token_type_ids_a': batch_a['token_type_ids'],\n",
    "            'attention_mask_a': batch_a['attention_mask'],\n",
    "            'input_ids_b': batch_b['input_ids'],\n",
    "            'token_type_ids_b': batch_b['token_type_ids'],\n",
    "            'attention_mask_b': batch_b['attention_mask'],\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=16)\n",
    "data_collator = CustomDataCollator(tokenizer)\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     MODEL_NAME,\n",
    "#     num_labels=NUM_LABELS\n",
    "# )\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = CustomDebertaSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, config=config\n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a76c0bc8f44cba8911ef35ceb02db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/57477 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fac0fea3098427784e4dae4b462bca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/57477 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def tokenize(examples, max_token_length: int):\n",
    "#     separator = \" [SEP] \"\n",
    "    \n",
    "#     joined_text = (\n",
    "#         examples[\"last_prompt\"]\n",
    "#         + separator\n",
    "#         + examples[\"last_response_a\"]\n",
    "#         + separator\n",
    "#         + examples[\"last_response_b\"]\n",
    "#     )\n",
    "\n",
    "#     tokenized = tokenizer(\n",
    "#         joined_text,\n",
    "#         max_length=max_token_length,\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",\n",
    "#     )\n",
    "\n",
    "    # return tokenizer(\n",
    "    #     joined_text,\n",
    "    #     max_length=max_token_length,\n",
    "    #     truncation=True,\n",
    "    #     padding=\"max_length\",\n",
    "    # )\n",
    "\n",
    "def tokenize(examples, suffix, max_token_length: int):\n",
    "    separator = \" [SEP] \"\n",
    "    \n",
    "    # TODO: 最後以外の応答も追加できるようにする\n",
    "    joined_text = (\n",
    "        examples[\"last_prompt\"]\n",
    "        + separator\n",
    "        + examples[f\"last_response_{suffix}\"]\n",
    "    )\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        joined_text,\n",
    "        max_length=max_token_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    return {key + f\"_{suffix}\": value for key, value in tokenized.items()}\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize,\n",
    "    batched=False,\n",
    "    fn_kwargs={\n",
    "        \"suffix\": \"a\",\n",
    "        \"max_token_length\": TRAINING_MAX_LENGTH\n",
    "    },\n",
    "    num_proc=NUM_PROC,\n",
    ").map(\n",
    "    tokenize,\n",
    "    batched=False,\n",
    "    fn_kwargs={\n",
    "        \"suffix\": \"b\",\n",
    "        \"max_token_length\": TRAINING_MAX_LENGTH\n",
    "    },\n",
    "    num_proc=NUM_PROC,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie', 'len_prompt', 'len_response_a', 'len_response_b', 'last_prompt', 'last_response_a', 'last_response_b', 'label', 'input_ids_a', 'token_type_ids_a', 'attention_mask_a', 'input_ids_b', 'token_type_ids_b', 'attention_mask_b'],\n",
       "    num_rows: 57477\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc9e16025ed4efdae014f7713d7a52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/57477 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def to_train_valid(ds):\n",
    "    return DatasetDict({\"train\": ds[\"train\"], \"valid\": ds[\"test\"]})\n",
    "\n",
    "train_dataset = train_dataset.cast_column('label', ClassLabel(num_classes=NUM_LABELS))\n",
    "\n",
    "train_valid_dataset = to_train_valid(\n",
    "    (\n",
    "        train_dataset\n",
    "        .train_test_split(\n",
    "            test_size=VALID_DATA_SIZE,\n",
    "            seed=SEED,\n",
    "            stratify_by_column=\"label\"\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie', 'len_prompt', 'len_response_a', 'len_response_b', 'last_prompt', 'last_response_a', 'last_response_b', 'label', 'input_ids_a', 'token_type_ids_a', 'attention_mask_a', 'input_ids_b', 'token_type_ids_b', 'attention_mask_b'],\n",
       "        num_rows: 40233\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['id', 'model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie', 'len_prompt', 'len_response_a', 'len_response_b', 'last_prompt', 'last_response_a', 'last_response_b', 'label', 'input_ids_a', 'token_type_ids_a', 'attention_mask_a', 'input_ids_b', 'token_type_ids_b', 'attention_mask_b'],\n",
       "        num_rows: 17244\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds_prob = softmax(predictions, axis=-1)\n",
    "    return {\"log_loss\": log_loss(labels, preds_prob)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ConcatDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "\n",
    "        input_ids_a = torch.tensor(item['input_ids_a'])\n",
    "        token_type_ids_a = torch.tensor(item['token_type_ids_a'])\n",
    "        attention_mask_a = torch.tensor(item['attention_mask_a'])\n",
    "        \n",
    "        input_ids_b = torch.tensor(item['input_ids_b'])\n",
    "        token_type_ids_b = torch.tensor(item['token_type_ids_b'])\n",
    "        attention_mask_b = torch.tensor(item['attention_mask_b'])\n",
    "        \n",
    "        label = torch.tensor(item['label'])\n",
    "\n",
    "        return {\n",
    "            'input_ids_a': input_ids_a,\n",
    "            'token_type_ids_a': token_type_ids_a,\n",
    "            'attention_mask_a': attention_mask_a,\n",
    "            'input_ids_b': input_ids_b,\n",
    "            'token_type_ids_b': token_type_ids_b,\n",
    "            'attention_mask_b': attention_mask_b,\n",
    "            'labels': label\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スケジューラの設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_PATH,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=32,\n",
    "    # per_device_train_batch_size=TRAIN_BS,\n",
    "    per_device_eval_batch_size=32,\n",
    "    # per_device_eval_batch_size=EVAL_BS,\n",
    "    # gradient_accumulation_steps=GRAD_ACC_NUM,\n",
    "    # eval_accumulation_steps=GRAD_ACC_NUM,\n",
    "    num_train_epochs=EPOCH,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=0.1,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=2,\n",
    "    seed=SEED,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    report_to=REPORT_TO,\n",
    "    run_name=EXP_NAME,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    # train_dataset=train_valid_dataset[\"train\"],\n",
    "    train_dataset=ConcatDataset(train_valid_dataset[\"train\"]),\n",
    "    # eval_dataset=train_valid_dataset[\"valid\"],\n",
    "    eval_dataset=ConcatDataset(train_valid_dataset[\"valid\"]),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='3774' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  68/3774 05:22 < 5:01:30, 0.20 it/s, Epoch 0.05/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if TRAINING:\n",
    "    # モデルの学習\n",
    "    trainer.train(\n",
    "        resume_from_checkpoint = RESUME_FROM_CHECKPOINT if RESUME_FROM_CHECKPOINT else None\n",
    "    )\n",
    "    # ログの保存に利用したストレージを削除\n",
    "    os.system(f\"rm -rf {MODEL_OUTPUT_PATH}/checkpoint-*\")\n",
    "    # モデルの保存\n",
    "    trainer.save_model(MODEL_OUTPUT_PATH)\n",
    "else:\n",
    "    # TRAINED_MODEL_PATHを用いて、学習済のモデルを読み込む\n",
    "    # model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    #     # TRAINED_MODEL_PATH,\n",
    "    #     \"/notebooks/lmsys/trained_models/e006-use-concat\",\n",
    "    #     num_labels=NUM_LABELS,\n",
    "    # )\n",
    "    model = CustomDebertaSequenceClassification.from_pretrained(\n",
    "        # MODEL_NAME\n",
    "        \"/notebooks/lmsys/trained_models/e006-use-concat\",\n",
    "    )\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        \".\",\n",
    "        per_device_eval_batch_size=4,\n",
    "        report_to=\"none\",\n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# valid_datasetの作成・保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # TRAININGをINFERRENCEでMAX_TOKENを変えるために、validを作り直す\n",
    "# valid_dataset = train_dataset.filter(\n",
    "#     lambda example: example[\"id\"]\n",
    "#     in train_valid_dataset[\"valid\"][\"id\"],\n",
    "#     num_proc=NUM_PROC,\n",
    "# )\n",
    "\n",
    "# # valid_dataset = valid_dataset.map(\n",
    "# #     tokenize,\n",
    "# #     batched=False,\n",
    "# #     fn_kwargs={\"max_token_length\": INFERENCE_MAX_LENGTH},\n",
    "# #     num_proc=NUM_PROC,\n",
    "# # )\n",
    "\n",
    "# valid_dataset = valid_dataset.map(\n",
    "#     tokenize,\n",
    "#     batched=False,\n",
    "#     fn_kwargs={\n",
    "#         \"suffix\": \"a\",\n",
    "#         \"max_token_length\": INFERENCE_MAX_LENGTH\n",
    "#     },\n",
    "#     num_proc=NUM_PROC,\n",
    "# ).map(\n",
    "#     tokenize,\n",
    "#     batched=False,\n",
    "#     fn_kwargs={\n",
    "#         \"suffix\": \"b\",\n",
    "#         \"max_token_length\": INFERENCE_MAX_LENGTH\n",
    "#     },\n",
    "#     num_proc=NUM_PROC,\n",
    "# )\n",
    "\n",
    "def add_valid_pred(example, idx, valid_pred):\n",
    "    example[\"valid_pred\"] = valid_pred[idx]\n",
    "    return example\n",
    "\n",
    "valid_dataset = train_valid_dataset[\"valid\"]\n",
    "\n",
    "valid_pred = softmax(\n",
    "    trainer.predict(ConcatDataset(valid_dataset)).predictions, axis=-1\n",
    ")\n",
    "\n",
    "np.save(f\"{MODEL_OUTPUT_PATH}/valid_prediction.npy\", valid_pred)\n",
    "\n",
    "valid_dataset = valid_dataset.map(\n",
    "    add_valid_pred,\n",
    "    with_indices=True,\n",
    "    fn_kwargs={\"valid_pred\": valid_pred}\n",
    ")\n",
    "\n",
    "valid_dataset.save_to_disk(f\"{MODEL_OUTPUT_PATH}/valid_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score = log_loss(valid_dataset[\"label\"], valid_pred)\n",
    "print(f\"CV Score: {cv_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_textを保存\n",
    "with open(f\"{MODEL_OUTPUT_PATH}/cv_score.txt\", \"w\") as f:\n",
    "    f.write(str(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWSへのアップロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# S3へのアップロード\n",
    "if not DEBUG and UPLOAD_DATA_TO_S3:\n",
    "    # uninstall\n",
    "    !sudo rm /usr/bin/aws\n",
    "    !sudo rm /usr/bin/aws_completer\n",
    "    !sudo rm -rf /usr/local/aws-cli\n",
    "\n",
    "    # install\n",
    "    !curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
    "    !unzip -o -qq awscliv2.zip\n",
    "    !sudo ./aws/install --update\n",
    "\n",
    "    # upload\n",
    "    output_name = MODEL_OUTPUT_PATH.split(\"/\")[-1]\n",
    "    os.system(\n",
    "        f\"aws s3 cp --recursive {MODEL_OUTPUT_PATH} s3://{COMPETITION_NAME}/trained_model/{output_name}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ダウンロード（参考）\n",
    "# !sudo rm /usr/bin/aws\n",
    "# !sudo rm /usr/bin/aws_completer\n",
    "# !sudo rm -rf /usr/local/aws-cli\n",
    "\n",
    "# !curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
    "# !unzip -o -qq awscliv2.zip\n",
    "# !sudo ./aws/install --update\n",
    "\n",
    "# !aws s3 cp --recursive s3://automated-essay-scoring/trained_model/e005-regression /notebooks/automated_essay_scoring/trained_models/e005-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Datasetへのupload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.system(\"mkdir -p ~/.kaggle/\")\n",
    "os.system(f\"cp /notebooks/{COMPETITION_NAME}/kaggle.json ~/.kaggle/\")\n",
    "os.system(\"chmod 600 ~/.kaggle/kaggle.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not DEBUG and UPLOAD_DATA_TO_KAGGLE:\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "    def dataset_create_new(dataset_name: str, upload_dir: str):\n",
    "        # if \"_\" in dataset_name:\n",
    "        #     raise ValueError(\"datasetの名称に_の使用は禁止です\")\n",
    "        dataset_metadata = {}\n",
    "        dataset_metadata[\"id\"] = f\"sinchir0/{dataset_name}\"\n",
    "        dataset_metadata[\"licenses\"] = [{\"name\": \"CC0-1.0\"}]\n",
    "        dataset_metadata[\"title\"] = dataset_name\n",
    "        with open(os.path.join(upload_dir, \"dataset-metadata.json\"), \"w\") as f:\n",
    "            json.dump(dataset_metadata, f, indent=4)\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode=\"tar\")\n",
    "\n",
    "    print(f\"Create Dataset name:{DATASET_NAME}, output_dir:{MODEL_OUTPUT_PATH}\")\n",
    "    dataset_create_new(dataset_name=DATASET_NAME, upload_dir=MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ローカルからのデータの削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (UPLOAD_DATA_TO_S3 or UPLOAD_DATA_TO_KAGGLE):\n",
    "    # ローカルからは削除\n",
    "    os.system(f\"rm -rf {MODEL_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if WANDB:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"finish Notebook!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
